{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8p07NOo22Bl"
   },
   "source": [
    "# Multilabel model\n",
    "Finding the best model for classification of multiple emotions for tekst messages. Using LSTM, CNN and Transformer models. \\\\\n",
    "Multi-Class Emotion Detection Model using Deep Learning\n",
    "\n",
    "This implementation builds and compares multiple neural network architectures\n",
    "for multi-label emotion classification using the GoEmotions dataset.\n",
    "\n",
    "Main references:\n",
    "[1] Demszky, D., Movshovitz-Attias, D., Ko, J., Cowen, A., Nemade, G., & Ravi, S. (2020).\\\\\n",
    "    \"GoEmotions: A Dataset of Fine-Grained Emotions.\" arXiv preprint arXiv:2005.00547.\\\\\n",
    "    https://github.com/google-research/google-research/tree/master/goemotions\n",
    "\n",
    "[2] Kim, Y. (2014). \"Convolutional Neural Networks for Sentence Classification.\"\n",
    "    Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n",
    "    https://aclanthology.org/D14-1181/\n",
    "\n",
    "[3] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L.,\n",
    "    & Polosukhin, I. (2017). \"Attention is all you need.\" Advances in neural information processing systems.\n",
    "    https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "\n",
    "[4] Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017).\n",
    "    \"Focal loss for dense object detection.\"\n",
    "    Proceedings of the IEEE international conference on computer vision.\n",
    "    https://arxiv.org/abs/1708.02002\n",
    "\n",
    "[5] Lipton, Z. C., Elkan, C., & Naryanaswamy, B. (2014).\n",
    "    \"Optimal thresholding of classifiers to maximize F1 measure.\"\n",
    "    Joint European Conference on Machine Learning and Knowledge Discovery in Databases.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yunhB9QK5Vxu"
   },
   "source": [
    "## preparing the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11553,
     "status": "ok",
     "timestamp": 1747836691059,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "LztZlz7h5dQ4",
    "outputId": "51899686-eb3a-478b-b920-7bef0e1154cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Conv1D\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import random\n",
    "import os\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yU1l3lf3H9S"
   },
   "source": [
    "## Loading dataset\n",
    "Getting the GoEmotions dataset from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10464,
     "status": "ok",
     "timestamp": 1747836705681,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "xd1-5PHyk-mY",
    "outputId": "fc25b0f2-195a-4697-aec1-b7e54c28c4fa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://gresearch/goemotions/data/full_dataset/goemotions_1.csv...\n",
      "/ [0 files][    0.0 B/ 13.5 MiB]                                                \n",
      "/ [0 files][320.0 KiB/ 13.5 MiB]                                                \n",
      "-\n",
      "\\\n",
      "\\ [1 files][ 13.5 MiB/ 13.5 MiB]                                                \n",
      "Copying gs://gresearch/goemotions/data/full_dataset/goemotions_2.csv...\n",
      "\\ [1 files][ 13.5 MiB/ 27.0 MiB]                                                \n",
      "\\ [1 files][ 13.8 MiB/ 27.0 MiB]                                                \n",
      "|\n",
      "/\n",
      "/ [2 files][ 27.0 MiB/ 27.0 MiB]                                                \n",
      "Copying gs://gresearch/goemotions/data/full_dataset/goemotions_3.csv...\n",
      "/ [2 files][ 27.0 MiB/ 40.8 MiB]                                                \n",
      "/ [2 files][ 27.4 MiB/ 40.8 MiB]                                                \n",
      "-\n",
      "\\\n",
      "\\ [3 files][ 40.8 MiB/ 40.8 MiB]                                                \n",
      "\n",
      "Operation completed over 3 objects/40.8 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Download the goemotions dataset\n",
    "!gsutil cp -r gs://gresearch/goemotions/data/full_dataset/ .\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv(\"full_dataset/goemotions_1.csv\")\n",
    "df2 = pd.read_csv(\"full_dataset/goemotions_2.csv\")\n",
    "df3 = pd.read_csv(\"full_dataset/goemotions_3.csv\")\n",
    "\n",
    "# Concatenate them into one dataframe\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# To reduce the runtime we use a random sample of 50.000 datapoints\n",
    "df_small = df.sample(n=50000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a9Vnl0K5tW8"
   },
   "source": [
    "## EDA\n",
    "We can see that the dataset is unbalanced, and some emotions are not represented a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1747313385659,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "j1cNkfh75wEm",
    "outputId": "52a801f3-bdf0-4c8e-a128-06126eb16f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50000, 37)\n",
      "Number of emotion labels: 28\n",
      "Emotion labels: ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      "\n",
      "Emotion distribution:\n",
      "neutral           13170\n",
      "admiration         4148\n",
      "approval           4042\n",
      "annoyance          3223\n",
      "gratitude          2738\n",
      "disapproval        2734\n",
      "curiosity          2286\n",
      "amusement          2241\n",
      "optimism           2118\n",
      "realization        2037\n",
      "disappointment     1998\n",
      "love               1935\n",
      "anger              1934\n",
      "joy                1866\n",
      "confusion          1728\n",
      "sadness            1549\n",
      "caring             1352\n",
      "excitement         1314\n",
      "disgust            1268\n",
      "surprise           1267\n",
      "desire              910\n",
      "fear                733\n",
      "remorse             608\n",
      "embarrassment       607\n",
      "nervousness         434\n",
      "pride               317\n",
      "relief              313\n",
      "grief               141\n",
      "dtype: int64\n",
      "\n",
      "Texts with no emotion labels: 807 (1.61%)\n"
     ]
    }
   ],
   "source": [
    "# Display basic info about the dataset\n",
    "print(f\"Dataset shape: {df_small.shape}\")\n",
    "\n",
    "# Identify emotion columns\n",
    "emotion_columns = df_small.columns[df_small.columns.get_loc('example_very_unclear') + 1:].tolist()\n",
    "print(f\"Number of emotion labels: {len(emotion_columns)}\")\n",
    "print(f\"Emotion labels: {emotion_columns}\")\n",
    "\n",
    "# Check label distribution\n",
    "label_counts = df_small[emotion_columns].sum().sort_values(ascending=False)\n",
    "print(\"\\nEmotion distribution:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Emotion Distribution in Dataset')\n",
    "plt.tight_layout()\n",
    "plt.savefig('emotion_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# Check for texts with no emotion labels\n",
    "no_emotion = (df_small[emotion_columns].sum(axis=1) == 0).sum()\n",
    "print(f\"\\nTexts with no emotion labels: {no_emotion} ({no_emotion/len(df_small)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri3Y9v1I6nNh"
   },
   "source": [
    "## Preprocessing data\n",
    "Deviding the data in a test and training set. \\\\\n",
    "Text preprocessing with tokenization and padding.We use tokenization to convert raw text into numbers, and padding to make all input sequences the same length — because neural networks don’t understand text and can’t process variable-length input.\n",
    "Based on techniques described in the Keras documentation:\n",
    "https://keras.io/api/preprocessing/text/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3586,
     "status": "ok",
     "timestamp": 1747313389247,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "FW-mBFvX9457",
    "outputId": "bbc7d209-e6ac-48fc-e1e9-3b449529a1af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 23655\n",
      "Padded Training Data Shape: (40000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "# Define features and target\n",
    "X = df_small['text']\n",
    "y = df_small[emotion_columns].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 10000  # Max vocabulary size\n",
    "max_len = 100  # Max sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>') #any word that exceeds the worklimit will be set by that oov token.\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Vocabulary Size: {len(tokenizer.word_index)}\")\n",
    "print(f\"Padded Training Data Shape: {X_train_pad.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFZmo499-06k"
   },
   "source": [
    "## Handling imbalance\n",
    "\"\"\"\n",
    "Calculate class weights to handle imbalance.\n",
    "This helps address the class imbalance issue common in emotion datasets.\n",
    "Approach adapted from scikit-learn's class_weight='balanced' principle.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1747313389264,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "2nA4sDfL_UsR",
    "outputId": "3c91a6e8-0d47-47c7-d220-1bf5c8b91613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(0.4293872643737387), 1: np.float64(0.7963051441312311), 2: np.float64(0.934317481080071), 3: np.float64(0.5597850425436632), 4: np.float64(0.4411894467484338), 5: np.float64(1.3190871916633689), 6: np.float64(1.0299721907508497), 7: np.float64(0.7785130400934216), 8: np.float64(1.9786307874950535), 9: np.float64(0.9070294784580499), 10: np.float64(0.6580246101204185), 11: np.float64(1.3950892857142858), 12: np.float64(2.886002886002886), 13: np.float64(1.3605442176870748), 14: np.float64(2.4461839530332683), 15: np.float64(0.6586313640255549), 16: 10.0, 17: np.float64(0.9523809523809523), 18: np.float64(0.9404683532399135), 19: np.float64(4.214075010535187), 20: np.float64(0.833472245374229), 21: np.float64(5.2521008403361344), 22: np.float64(0.8689607229753216), 23: np.float64(5.668934240362812), 24: np.float64(3.007518796992481), 25: np.float64(1.1690437222352117), 26: np.float64(1.411631846414455), 27: np.float64(0.13487268018990073)}\n"
     ]
    }
   ],
   "source": [
    "class_counts = np.sum(y_train, axis=0)\n",
    "total_samples = len(y_train)\n",
    "class_weights = {}\n",
    "for i in range(len(emotion_columns)):\n",
    "    # Calculate weight as inverse of class frequency, normalized\n",
    "    weight = total_samples / (len(emotion_columns) * class_counts[i])\n",
    "    class_weights[i] = min(weight, 10.0)  # Cap the weight to prevent extreme values\n",
    "\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHKSpd4C_glj"
   },
   "source": [
    "## Defining loss function\n",
    "We add an extra penalty for all zero-predictions. Since predicting always 0 for all emotions will result in high metrics but a bad model. we want the model to predict emotions. \\\\\n",
    "We are using focal loss because that functions well with an inbalanced dataset.\n",
    "\"\"\"\n",
    "Custom focal loss with additional penalty for all-zero predictions.\n",
    "Based on Lin et al., 2017 [4] with our own extension to prevent all-zero predictions.\n",
    "\"\"\"\n",
    "\n",
    "**TODO check if the penalty can be deleted, since we also have other code that makes sure one emotion is predicted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "h1QG12tv__Zh"
   },
   "outputs": [],
   "source": [
    "def focal_loss_with_penalty(y_true, y_pred, gamma=2.0, alpha=0.25, epsilon=1e-7):\n",
    "    # Focal loss for multi-label classification\n",
    "    y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "    cross_entropy = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "\n",
    "    p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "    focal_loss = alpha * tf.pow(1 - p_t, gamma) * cross_entropy\n",
    "\n",
    "    # Add penalty for all-zero predictions\n",
    "    sum_pred = tf.reduce_sum(y_pred, axis=1)\n",
    "    all_zero_penalty = 5.0 * tf.exp(-sum_pred)  # Penalty increases as sum approaches zero\n",
    "\n",
    "    return tf.reduce_mean(focal_loss) + tf.reduce_mean(all_zero_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acTr8Uo-BCm2"
   },
   "source": [
    "## Define F1 score\n",
    "We use the F1 score as metric for optimizing the models. F1 micro is best for our multi-label emotion detection model because it accounts for class imbalance by aggregating all true positives, false positives, and false negatives across all labels, giving a more realistic measure of overall model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qZZIRQFSBWjy"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Custom F1 score metric for Keras.\n",
    "Implements F1 score calculation directly in TensorFlow for use during training.\n",
    "\"\"\"\n",
    "def f1_metric(y_true, y_pred):\n",
    "    y_pred_binary = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n",
    "\n",
    "    true_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred_binary, 1)), tf.float32))\n",
    "    false_positives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 0), tf.equal(y_pred_binary, 1)), tf.float32))\n",
    "    false_negatives = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, 1), tf.equal(y_pred_binary, 0)), tf.float32))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + tf.keras.backend.epsilon())\n",
    "    recall = true_positives / (true_positives + false_negatives + tf.keras.backend.epsilon())\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdT2zZZYBchh"
   },
   "source": [
    "## prediction function\n",
    "Makes sure at least one emotion is predicted, by predicting the one with the highest probability.\n",
    "Based on the principles discussed in Lipton et al., 2014 [5] with our adaptation\n",
    "for multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6A4W8IcMBscF"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Threshold-based prediction function that ensures at least one emotion is predicted.\n",
    "Addresses the requirement to prevent all-zero predictions.\n",
    "Based on the principles discussed in Lipton et al., 2014 [5] with our adaptation\n",
    "for multi-label classification.\n",
    "\"\"\"\n",
    "def predict_with_threshold(model, X, threshold=0.5):\n",
    "    y_pred_proba = model.predict(X)\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # If any row has all zeros, set the highest probability class to 1\n",
    "    zero_rows = np.where(np.sum(y_pred, axis=1) == 0)[0]\n",
    "    for row in zero_rows:\n",
    "        max_prob_idx = np.argmax(y_pred_proba[row])\n",
    "        y_pred[row, max_prob_idx] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6akbyFgsByWw"
   },
   "source": [
    "## LSTM model\n",
    "\n",
    "\"\"\"\n",
    "Model 1: Bidirectional LSTM for text classification.\n",
    "Based on architecture described in:\n",
    "Schuster, M., & Paliwal, K. K. (1997). \"Bidirectional recurrent neural networks.\"\n",
    "IEEE transactions on Signal Processing, 45(11), 2673-2681.\n",
    "\"\"\"\n",
    "\n",
    "**TODO: we can still check if optimizing the parameters helps. But the Transformer model is the best anyways.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FzQVw9UtB5Dc"
   },
   "outputs": [],
   "source": [
    "def build_lstm_model(vocab_size, embedding_dim=100, lstm_units=128, dropout_rate=0.5):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(len(emotion_columns), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=focal_loss_with_penalty,\n",
    "        metrics=['accuracy', f1_metric]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhZvqALDC4yH"
   },
   "source": [
    "## CNN model\n",
    "\"\"\"\n",
    "Model 2: 1D CNN for text classification.\n",
    "Implements a multi-filter CNN architecture inspired by:\n",
    "Kim, Y. (2014). \"Convolutional Neural Networks for Sentence Classification.\" [2]\n",
    "\"\"\"\n",
    "\n",
    "***TODO: optimize parameters, but transformer model is going to be better anyway***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "T6xieCbnDLLx"
   },
   "outputs": [],
   "source": [
    "def build_cnn_model(vocab_size, embedding_dim=100, filters=128, kernel_size=5, dropout_rate=0.5):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "\n",
    "    # Multiple conv layers with different kernel sizes\n",
    "    conv1 = Conv1D(filters, 3, padding='same', activation='relu')(x)\n",
    "    conv2 = Conv1D(filters, 4, padding='same', activation='relu')(x)\n",
    "    conv3 = Conv1D(filters, 5, padding='same', activation='relu')(x)\n",
    "\n",
    "    # Pool each conv layer separately\n",
    "    pool1 = GlobalMaxPooling1D()(conv1)\n",
    "    pool2 = GlobalMaxPooling1D()(conv2)\n",
    "    pool3 = GlobalMaxPooling1D()(conv3)\n",
    "\n",
    "    # Concatenate the pooled features\n",
    "    concat = tf.keras.layers.concatenate([pool1, pool2, pool3])\n",
    "    x = Dense(256, activation='relu')(concat)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(len(emotion_columns), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=focal_loss_with_penalty,\n",
    "        metrics=['accuracy', f1_metric]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZiyKOqvDQRk"
   },
   "source": [
    "## Transformer model\n",
    "\"\"\"\n",
    "Model 3: Transformer-based model for text classification.\n",
    "Simplified implementation inspired by:\n",
    "Vaswani, A., et al. (2017). \"Attention is all you need.\" [3]\n",
    "\"\"\"\n",
    "\n",
    "We also added positional embedding, since that gives the transformer model information about the position of the words.\n",
    "the original method introduced in the \"Attention Is All You Need\" paper by Vaswani et al. (2017), which uses sine and cosine functions of varying frequencies to encode positional information. This approach allows the Transformer model to capture the order of words in a sequence, which is crucial since Transformers lack inherent sequential processing capabilities. \\\\\n",
    "Source of code for positional encoding is from Miguel. \\\\\n",
    "Since the positional embedding gave us worse metrics in the results we added learning positional embedding. Where just like the normal embedding layer the parameters are learned and optimized. \\\\\n",
    "***TODO: optimize parameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NebQOpOPxxXQ"
   },
   "outputs": [],
   "source": [
    "#normal positional encoding\n",
    "def get_positional_encoding(seq_length, d_model):\n",
    "    # Calculate positional encoding\n",
    "    positions = np.arange(seq_length)[:, np.newaxis]\n",
    "    depths = np.arange(d_model)[np.newaxis, :] // 2 * 2  # Integer division\n",
    "\n",
    "    # Create angle rates\n",
    "    angle_rates = 1 / np.power(10000, (2 * (depths // 2)) / np.float32(d_model))\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    # Apply sin/cos to even/odd indices\n",
    "    pos_encoding = np.zeros(angle_rads.shape)\n",
    "    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NlTKBeCN0oKW"
   },
   "outputs": [],
   "source": [
    "#working model without positional embedding\n",
    "def build_transformer_model(vocab_size, embedding_dim=100, num_heads=8, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "\n",
    "    # Transformer block\n",
    "    transformer_block = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embedding_dim // num_heads\n",
    "    )(embedding_layer, embedding_layer)\n",
    "\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(transformer_block + embedding_layer)\n",
    "\n",
    "    # Feed Forward\n",
    "    ff = Dense(ff_dim, activation='relu')(x)\n",
    "    ff = Dense(embedding_dim)(ff)\n",
    "\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(len(emotion_columns), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=focal_loss_with_penalty,\n",
    "        metrics=['accuracy', f1_metric]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xCr5iZGfDsGK"
   },
   "outputs": [],
   "source": [
    "#working model with positional embedding\n",
    "def build_transformer_model(vocab_size, embedding_dim=100, num_heads=8, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    embedding_layer = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "\n",
    "    # Embedding layer for positional encoding\n",
    "    pos_encoding = get_positional_encoding(max_len,  embedding_dim)\n",
    "    pos_encoding = tf.keras.backend.constant(pos_encoding)\n",
    "\n",
    "    embedded_with_pos = embedding_layer + pos_encoding\n",
    "\n",
    "    # Transformer block\n",
    "    transformer_block = layers.MultiHeadAttention(\n",
    "        num_heads=2, key_dim=64\n",
    "    )(embedded_with_pos, embedded_with_pos)\n",
    "\n",
    "    norm_transformer_block = layers.LayerNormalization(epsilon=1e-6)(transformer_block + embedded_with_pos)\n",
    "\n",
    "    # Feed Forward\n",
    "    ff = Dense(ff_dim, activation='relu')(norm_transformer_block)\n",
    "    ff = Dense(embedding_dim)(ff)\n",
    "\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(norm_transformer_block + ff)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(len(emotion_columns), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=focal_loss_with_penalty,\n",
    "        metrics=['accuracy', f1_metric]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNhaUgc7HTWM"
   },
   "source": [
    "Learning positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PHwiEZqdHSQr"
   },
   "outputs": [],
   "source": [
    "class LearnablePositionalEncoding(layers.Layer):\n",
    "    def __init__(self, maxlen, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = self.add_weight(\n",
    "            shape=(maxlen, embedding_dim),\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name=\"learnable_pos_embedding\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape: (batch_size, sequence_length, embedding_dim)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pos_embedding[tf.newaxis, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HfbzmBMxHWdx"
   },
   "outputs": [],
   "source": [
    "#model with learnable positional encoding\n",
    "def build_transformer_model(vocab_size, embedding_dim=100, num_heads=8, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "\n",
    "    # Embedding + positional encoding\n",
    "    x = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = LearnablePositionalEncoding(maxlen=max_len, embedding_dim=embedding_dim)(x)\n",
    "\n",
    "    # Transformer block\n",
    "    attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embedding_dim // num_heads\n",
    "    )(x, x)\n",
    "\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_output + x)\n",
    "\n",
    "    # Feed Forward\n",
    "    ff = Dense(ff_dim, activation='relu')(x)\n",
    "    ff = Dense(embedding_dim)(ff)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x + ff)\n",
    "\n",
    "    # Classification head\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(len(emotion_columns), activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=focal_loss_with_penalty,\n",
    "        metrics=['accuracy', f1_metric]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eYC5vdaDw48"
   },
   "source": [
    "## Callbakcs\n",
    "Introduce early stopping so the model will stop after 3 epochs after eachother do not improve the f1 score. The vest model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bkTZktcsEBQr"
   },
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_f1_metric', patience=4, mode='max', restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_f1_metric', mode='max', save_best_only=True)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPwQYOamD-0a"
   },
   "source": [
    "## Training and evaluating the models\n",
    "\n",
    "***TODO: check parameters like epochs, batchsize***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-QpJpcBZKJ7n"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, model_name):\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred = predict_with_threshold(model, X_test_pad, threshold=0.5)\n",
    "\n",
    "    # Calculate F1 scores\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    micro_f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "    print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
    "\n",
    "    # Check if any all-zero predictions remain\n",
    "    zero_preds = (np.sum(y_pred, axis=1) == 0).sum()\n",
    "    print(f\"Texts with no emotion predictions: {zero_preds} ({zero_preds/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['f1_metric'], label='Train F1')\n",
    "    plt.plot(history.history['val_f1_metric'], label='Val F1')\n",
    "    plt.title(f'{model_name} - F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_training_history.png')\n",
    "    plt.close()\n",
    "\n",
    "    return model, macro_f1, micro_f1, weighted_f1, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uumtXN97KNQZ"
   },
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 996
    },
    "executionInfo": {
     "elapsed": 174184,
     "status": "ok",
     "timestamp": 1747313563669,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "1VlCIi0hKP1H",
    "outputId": "1c491e7e-d5a0-420a-d3a5-251dd7f1f8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building LSTM model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,196</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m234,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)             │         \u001b[38;5;34m7,196\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,307,484</span> (4.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,307,484\u001b[0m (4.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,307,484</span> (4.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,307,484\u001b[0m (4.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training LSTM ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.2430 - f1_metric: 0.0742 - loss: 0.0119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 130ms/step - accuracy: 0.2431 - f1_metric: 0.0742 - loss: 0.0119 - val_accuracy: 0.3685 - val_f1_metric: 0.2309 - val_loss: 0.0132\n",
      "Epoch 2/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.3681 - f1_metric: 0.2235 - loss: 0.0100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 120ms/step - accuracy: 0.3681 - f1_metric: 0.2235 - loss: 0.0100 - val_accuracy: 0.3882 - val_f1_metric: 0.3036 - val_loss: 0.0129\n",
      "Epoch 3/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.3937 - f1_metric: 0.3040 - loss: 0.0097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 112ms/step - accuracy: 0.3937 - f1_metric: 0.3040 - loss: 0.0097 - val_accuracy: 0.3901 - val_f1_metric: 0.3259 - val_loss: 0.0129\n",
      "Epoch 4/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.4242 - f1_metric: 0.3578 - loss: 0.0094"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 115ms/step - accuracy: 0.4242 - f1_metric: 0.3578 - loss: 0.0094 - val_accuracy: 0.3831 - val_f1_metric: 0.3318 - val_loss: 0.0129\n",
      "Epoch 5/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.4599 - f1_metric: 0.4079 - loss: 0.0091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 129ms/step - accuracy: 0.4599 - f1_metric: 0.4079 - loss: 0.0091 - val_accuracy: 0.3691 - val_f1_metric: 0.3369 - val_loss: 0.0131\n",
      "Epoch 6/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 125ms/step - accuracy: 0.5018 - f1_metric: 0.4609 - loss: 0.0088 - val_accuracy: 0.3577 - val_f1_metric: 0.3335 - val_loss: 0.0134\n",
      "Epoch 7/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 117ms/step - accuracy: 0.5393 - f1_metric: 0.5101 - loss: 0.0086 - val_accuracy: 0.3471 - val_f1_metric: 0.3309 - val_loss: 0.0138\n",
      "Epoch 8/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 119ms/step - accuracy: 0.5740 - f1_metric: 0.5551 - loss: 0.0083 - val_accuracy: 0.3338 - val_f1_metric: 0.3263 - val_loss: 0.0142\n",
      "Epoch 9/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 121ms/step - accuracy: 0.5989 - f1_metric: 0.5923 - loss: 0.0081 - val_accuracy: 0.3371 - val_f1_metric: 0.3280 - val_loss: 0.0151\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 57ms/step\n",
      "\n",
      "LSTM Results:\n",
      "Macro F1 Score: 0.2409\n",
      "Micro F1 Score: 0.3773\n",
      "Weighted F1 Score: 0.3275\n",
      "Texts with no emotion predictions: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train LSTM model\n",
    "print(\"\\nBuilding LSTM model...\")\n",
    "lstm_model = build_lstm_model(vocab_size=min(len(tokenizer.word_index) + 1, max_words))\n",
    "lstm_model.summary()\n",
    "lstm_model, lstm_macro_f1, lstm_micro_f1, lstm_weighted_f1, lstm_preds = train_and_evaluate(lstm_model, \"LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5D8KZguKT9m"
   },
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 86720,
     "status": "ok",
     "timestamp": 1747313650393,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "7JDcOSLuKVzj",
    "outputId": "2b8e2855-886f-468c-b855-1dc54c6f3384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building CNN model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">38,528</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,328</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,560</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,196</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │  \u001b[38;5;34m1,000,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m38,528\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m51,328\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m64,128\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
       "│                     │                   │            │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m98,560\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │      \u001b[38;5;34m7,196\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,740</span> (4.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,259,740\u001b[0m (4.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,740</span> (4.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,259,740\u001b[0m (4.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training CNN ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 30ms/step - accuracy: 0.2437 - f1_metric: 0.0803 - loss: 0.0121 - val_accuracy: 0.3756 - val_f1_metric: 0.2284 - val_loss: 0.0132\n",
      "Epoch 2/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15ms/step - accuracy: 0.3706 - f1_metric: 0.2286 - loss: 0.0101 - val_accuracy: 0.3891 - val_f1_metric: 0.3090 - val_loss: 0.0129\n",
      "Epoch 3/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 24ms/step - accuracy: 0.4010 - f1_metric: 0.3198 - loss: 0.0097 - val_accuracy: 0.3918 - val_f1_metric: 0.3284 - val_loss: 0.0129\n",
      "Epoch 4/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 42ms/step - accuracy: 0.4485 - f1_metric: 0.3879 - loss: 0.0093 - val_accuracy: 0.3751 - val_f1_metric: 0.3273 - val_loss: 0.0131\n",
      "Epoch 5/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 34ms/step - accuracy: 0.4997 - f1_metric: 0.4621 - loss: 0.0089 - val_accuracy: 0.3646 - val_f1_metric: 0.3342 - val_loss: 0.0134\n",
      "Epoch 6/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 40ms/step - accuracy: 0.5598 - f1_metric: 0.5330 - loss: 0.0085 - val_accuracy: 0.3544 - val_f1_metric: 0.3301 - val_loss: 0.0137\n",
      "Epoch 7/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 42ms/step - accuracy: 0.6017 - f1_metric: 0.5840 - loss: 0.0082 - val_accuracy: 0.3431 - val_f1_metric: 0.3202 - val_loss: 0.0140\n",
      "Epoch 8/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 45ms/step - accuracy: 0.6363 - f1_metric: 0.6267 - loss: 0.0080 - val_accuracy: 0.3359 - val_f1_metric: 0.3169 - val_loss: 0.0145\n",
      "Epoch 9/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38ms/step - accuracy: 0.6586 - f1_metric: 0.6610 - loss: 0.0078 - val_accuracy: 0.3280 - val_f1_metric: 0.3125 - val_loss: 0.0151\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "CNN Results:\n",
      "Macro F1 Score: 0.2385\n",
      "Micro F1 Score: 0.3710\n",
      "Weighted F1 Score: 0.3241\n",
      "Texts with no emotion predictions: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train CNN model\n",
    "print(\"\\nBuilding CNN model...\")\n",
    "cnn_model = build_cnn_model(vocab_size=min(len(tokenizer.word_index) + 1, max_words))\n",
    "cnn_model.summary()\n",
    "cnn_model, cnn_macro_f1, cnn_micro_f1, cnn_weighted_f1, cnn_preds = train_and_evaluate(cnn_model, \"CNN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4jtAUMBKYh0"
   },
   "source": [
    "### Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 77303,
     "status": "ok",
     "timestamp": 1747313727700,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "zeFO9iO5Kas3",
    "outputId": "b5c89d25-2c60-4fef-9e1c-63e620981a9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Transformer model...\n",
      "WARNING:tensorflow:From c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ learnable_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,000</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LearnablePosition…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">38,788</span> │ learnable_positi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ learnable_positi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "│                     │                   │            │ learnable_positi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,928</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,900</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,856</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,196</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │  \u001b[38;5;34m1,000,000\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ learnable_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │     \u001b[38;5;34m10,000\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mLearnablePosition…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │     \u001b[38;5;34m38,788\u001b[0m │ learnable_positi… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ learnable_positi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "│                     │                   │            │ learnable_positi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │        \u001b[38;5;34m200\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m12,928\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │     \u001b[38;5;34m12,900\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │        \u001b[38;5;34m200\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m25,856\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m)        │      \u001b[38;5;34m7,196\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,108,068</span> (4.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,108,068\u001b[0m (4.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,108,068</span> (4.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,108,068\u001b[0m (4.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Transformer ===\n",
      "Epoch 1/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 48ms/step - accuracy: 0.2231 - f1_metric: 0.0530 - loss: 0.0128 - val_accuracy: 0.3501 - val_f1_metric: 0.1559 - val_loss: 0.0135\n",
      "Epoch 2/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 25ms/step - accuracy: 0.3589 - f1_metric: 0.1860 - loss: 0.0101 - val_accuracy: 0.3865 - val_f1_metric: 0.3288 - val_loss: 0.0131\n",
      "Epoch 3/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 29ms/step - accuracy: 0.3928 - f1_metric: 0.2897 - loss: 0.0097 - val_accuracy: 0.3919 - val_f1_metric: 0.3251 - val_loss: 0.0130\n",
      "Epoch 4/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 57ms/step - accuracy: 0.4163 - f1_metric: 0.3429 - loss: 0.0095 - val_accuracy: 0.3837 - val_f1_metric: 0.3308 - val_loss: 0.0131\n",
      "Epoch 5/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 44ms/step - accuracy: 0.4484 - f1_metric: 0.3953 - loss: 0.0092 - val_accuracy: 0.3652 - val_f1_metric: 0.3319 - val_loss: 0.0132\n",
      "Epoch 6/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 35ms/step - accuracy: 0.4865 - f1_metric: 0.4440 - loss: 0.0089 - val_accuracy: 0.3561 - val_f1_metric: 0.3343 - val_loss: 0.0135\n",
      "Epoch 7/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 43ms/step - accuracy: 0.5243 - f1_metric: 0.4970 - loss: 0.0086 - val_accuracy: 0.3399 - val_f1_metric: 0.3224 - val_loss: 0.0138\n",
      "Epoch 8/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 41ms/step - accuracy: 0.5661 - f1_metric: 0.5527 - loss: 0.0084 - val_accuracy: 0.3290 - val_f1_metric: 0.3125 - val_loss: 0.0140\n",
      "Epoch 9/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.5955 - f1_metric: 0.5932 - loss: 0.0081 - val_accuracy: 0.3369 - val_f1_metric: 0.3320 - val_loss: 0.0146\n",
      "Epoch 10/10\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 32ms/step - accuracy: 0.6279 - f1_metric: 0.6376 - loss: 0.0079 - val_accuracy: 0.3366 - val_f1_metric: 0.3289 - val_loss: 0.0151\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step\n",
      "\n",
      "Transformer Results:\n",
      "Macro F1 Score: 0.2501\n",
      "Micro F1 Score: 0.3630\n",
      "Weighted F1 Score: 0.3234\n",
      "Texts with no emotion predictions: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train Transformer model\n",
    "print(\"\\nBuilding Transformer model...\")\n",
    "transformer_model = build_transformer_model(vocab_size=min(len(tokenizer.word_index) + 1, max_words))\n",
    "transformer_model.summary()\n",
    "transformer_model, transformer_macro_f1, transformer_micro_f1, transformer_weighted_f1, transformer_preds = train_and_evaluate(transformer_model, \"Transformer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvp1BE6mKdzk"
   },
   "source": [
    "## Comparing the models\n",
    "We use the micro F1 score to compare the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1747313727737,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "iRoACRH0Eh-5",
    "outputId": "1ab98127-3a01-4930-c84f-2625cbc10e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "         Model  Macro F1  Micro F1  Weighted F1\n",
      "0       BiLSTM  0.240904  0.377341     0.327515\n",
      "1          CNN  0.238546  0.371019     0.324115\n",
      "2  Transformer  0.250132  0.363032     0.323398\n",
      "\n",
      "Best model: BiLSTM with Micro F1: 0.3773\n"
     ]
    }
   ],
   "source": [
    "# Compare models\n",
    "models_comparison = {\n",
    "    'Model': ['BiLSTM', 'CNN', 'Transformer'],\n",
    "    'Macro F1': [lstm_macro_f1, cnn_macro_f1, transformer_macro_f1],\n",
    "    'Micro F1': [lstm_micro_f1, cnn_micro_f1, transformer_micro_f1],\n",
    "    'Weighted F1': [lstm_weighted_f1, cnn_weighted_f1, transformer_weighted_f1]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(models_comparison)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Identify best model\n",
    "best_idx = np.argmax([lstm_micro_f1, cnn_micro_f1, transformer_micro_f1])\n",
    "best_models = [lstm_model, cnn_model, transformer_model]\n",
    "best_model = best_models[best_idx]\n",
    "best_model_name = models_comparison['Model'][best_idx]\n",
    "\n",
    "print(f\"\\nBest model: {best_model_name} with Micro F1: {models_comparison['Micro F1'][best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJaYcZF7Luag"
   },
   "source": [
    "## Optimize threshold\n",
    "For the best model found above we optimize the threshold to maximize the f1 score. \\\\\n",
    "Based on the approach described in:\n",
    "Lipton, Z. C., Elkan, C., & Naryanaswamy, B. (2014) [5] \\\\\n",
    "We also check the f1 score per emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14282,
     "status": "ok",
     "timestamp": 1747313742024,
     "user": {
      "displayName": "Jasmijn",
      "userId": "03758189988651999849"
     },
     "user_tz": -60
    },
    "id": "qEJzRiB-MORc",
    "outputId": "c75ed288-bcb3-4963-8009-c7a8c7b83a6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizing threshold for best model...\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step\n",
      "Threshold: 0.20, Micro F1: 0.0813\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step\n",
      "Threshold: 0.25, Micro F1: 0.1046\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step\n",
      "Threshold: 0.30, Micro F1: 0.2350\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step\n",
      "Threshold: 0.35, Micro F1: 0.3371\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step\n",
      "Threshold: 0.40, Micro F1: 0.3884\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step\n",
      "Threshold: 0.45, Micro F1: 0.3881\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step\n",
      "Threshold: 0.50, Micro F1: 0.3773\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step\n",
      "Threshold: 0.55, Micro F1: 0.3731\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step\n",
      "Threshold: 0.60, Micro F1: 0.3729\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step\n",
      "Threshold: 0.65, Micro F1: 0.3726\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step\n",
      "Threshold: 0.70, Micro F1: 0.3725\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step\n",
      "Threshold: 0.75, Micro F1: 0.3725\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step\n",
      "Threshold: 0.80, Micro F1: 0.3725\n",
      "\n",
      "Best threshold: 0.40\n",
      "Best F1 score: 0.3884\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step\n",
      "\n",
      "Final Model Results with Optimized Threshold:\n",
      "Macro F1 Score: 0.2801\n",
      "Micro F1 Score: 0.3884\n",
      "Weighted F1 Score: 0.3612\n",
      "\n",
      "F1 Score by emotion:\n",
      "admiration: 0.4480\n",
      "amusement: 0.5677\n",
      "anger: 0.3493\n",
      "annoyance: 0.2602\n",
      "approval: 0.2094\n",
      "caring: 0.1823\n",
      "confusion: 0.1614\n",
      "curiosity: 0.2549\n",
      "desire: 0.2789\n",
      "disappointment: 0.1555\n",
      "disapproval: 0.2313\n",
      "disgust: 0.2259\n",
      "embarrassment: 0.0513\n",
      "excitement: 0.1574\n",
      "fear: 0.4154\n",
      "gratitude: 0.7797\n",
      "grief: 0.0000\n",
      "joy: 0.3566\n",
      "love: 0.6233\n",
      "nervousness: 0.0000\n",
      "optimism: 0.3529\n",
      "pride: 0.0000\n",
      "realization: 0.1208\n",
      "relief: 0.0000\n",
      "remorse: 0.5079\n",
      "sadness: 0.3543\n",
      "surprise: 0.3164\n",
      "neutral: 0.4825\n"
     ]
    }
   ],
   "source": [
    "def optimize_threshold(model, X, y_true, thresholds=None):\n",
    "    if thresholds is None:\n",
    "        thresholds = np.arange(0.2, 0.8, 0.05)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = predict_with_threshold(model, X, threshold=threshold)\n",
    "        micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "        results.append((threshold, micro_f1))\n",
    "        print(f\"Threshold: {threshold:.2f}, Micro F1: {micro_f1:.4f}\")\n",
    "\n",
    "    best_threshold, best_f1 = max(results, key=lambda x: x[1])\n",
    "    print(f\"\\nBest threshold: {best_threshold:.2f}\")\n",
    "    print(f\"Best F1 score: {best_f1:.4f}\")\n",
    "\n",
    "    # Plot thresholds vs F1 scores\n",
    "    thresholds, f1_scores = zip(*results)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, f1_scores, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Micro F1 Score')\n",
    "    plt.title('Threshold Optimization')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('threshold_optimization.png')\n",
    "    plt.close()\n",
    "\n",
    "    return best_threshold\n",
    "\n",
    "\"\"\"\n",
    "Threshold Optimization and Final Evaluation\n",
    "\"\"\"\n",
    "# Optimize threshold for best model\n",
    "print(\"\\nOptimizing threshold for best model...\")\n",
    "best_threshold = optimize_threshold(best_model, X_test_pad, y_test)\n",
    "\n",
    "# Get final predictions with optimized threshold\n",
    "final_preds = predict_with_threshold(best_model, X_test_pad, threshold=best_threshold)\n",
    "final_macro_f1 = f1_score(y_test, final_preds, average='macro')\n",
    "final_micro_f1 = f1_score(y_test, final_preds, average='micro')\n",
    "final_weighted_f1 = f1_score(y_test, final_preds, average='weighted')\n",
    "per_class_f1 = f1_score(y_test, final_preds, average=None)\n",
    "\n",
    "print(\"\\nFinal Model Results with Optimized Threshold:\")\n",
    "print(f\"Macro F1 Score: {final_macro_f1:.4f}\")\n",
    "print(f\"Micro F1 Score: {final_micro_f1:.4f}\")\n",
    "print(f\"Weighted F1 Score: {final_weighted_f1:.4f}\")\n",
    "\n",
    "# Print per-class F1 scores\n",
    "print(\"\\nF1 Score by emotion:\")\n",
    "for i, emotion in enumerate(emotion_columns):\n",
    "    print(f\"{emotion}: {per_class_f1[i]:.4f}\")\n",
    "\n",
    "# Visualize per-class F1 scores\n",
    "plt.figure(figsize=(14, 8))\n",
    "emotion_f1_dict = dict(zip(emotion_columns, per_class_f1))\n",
    "sorted_emotion_f1 = {k: v for k, v in sorted(emotion_f1_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "sns.barplot(x=list(sorted_emotion_f1.keys()), y=list(sorted_emotion_f1.values()))\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('F1 Score by Emotion')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('emotion_f1_scores.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to text experiment for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score  # For calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Whisper model\n",
    "whisper_model = whisper.load_model(\"base\")  # Use your desired model: \"base\", \"small\", \"medium\", or \"large\"\n",
    "\n",
    "# Load your sentiment analysis model (make sure it's trained and saved previously)\n",
    "# Example:\n",
    "# model = keras.models.load_model('path_to_your_sentiment_model.h5')\n",
    "\n",
    "# Path to the audio files folder\n",
    "audio_folder_path = 'Audio_Files/'\n",
    "emotion_data = pd.read_csv('labels_2.csv')\n",
    "\n",
    "def get_actual_emotion(filename):\n",
    "    # Look for the corresponding row for the filename in the CSV file\n",
    "    row = emotion_data[emotion_data['filename'] == filename]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['emotion']  # Return the emotion value from the third column\n",
    "    return None  # In case the file is not found\n",
    "\n",
    "\n",
    "def predict_emotions(texts, model=best_model, emotion_cols=emotion_columns, tokenizer=tokenizer, threshold=best_threshold):\n",
    "    # Preprocess the texts\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(padded_sequences)\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "\n",
    "    # Ensure at least one emotion is predicted for each text\n",
    "    zero_rows = np.where(np.sum(y_pred, axis=1) == 0)[0]\n",
    "    for row in zero_rows:\n",
    "        max_prob_idx = np.argmax(y_pred_proba[row])\n",
    "        y_pred[row, max_prob_idx] = 1\n",
    "\n",
    "    # Create results as a DataFrame\n",
    "    results = pd.DataFrame(y_pred, columns=emotion_cols)\n",
    "    results.insert(0, 'text', texts)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to process each audio file and predict emotion using LSTM model\n",
    "def process_audio(audio_path, audio_file):\n",
    "    try:\n",
    "        # Transcribe the audio file using Whisper\n",
    "        result = whisper_model.transcribe(audio_path)\n",
    "        transcribed_text = result['text']\n",
    "        print(f\"Transcription for {audio_file}: {transcribed_text}\")\n",
    "\n",
    "        predictions = predict_emotions([transcribed_text])\n",
    "        predicted_emotions_list = predictions.loc[:, (predictions != 0).any(axis=0)]\n",
    "        #get columnnames of predicted_emotions\n",
    "        predicted_emotion = predicted_emotions_list.columns[1:]\n",
    "\n",
    "        # Get the actual emotion for comparison\n",
    "        actual_emotion = get_actual_emotion(audio_file)\n",
    "\n",
    "        # Print the predicted emotion\n",
    "        print(f\"Predicted Emotion for {audio_file}: {predicted_emotion}\")\n",
    "\n",
    "        # Return the transcription, predicted emotion, and actual emotion\n",
    "        return transcribed_text, list(predicted_emotion), actual_emotion\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Whisper transcription for {audio_path}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# List to store predicted and actual emotions for evaluation\n",
    "predicted_emotions = []\n",
    "actual_emotions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio1.m4a:  That was a fantastic presentation. I'm really impressed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted Emotion for audio1.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio1.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio10.m4a:  After watching the performance, I couldn't help but admire the level of skill and dedication.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio10.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio10.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio100.m4a:  It's strange how something so familiar can bring so much sadness when it's gone.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio100.m4a: Index(['disappointment', 'fear', 'sadness'], dtype='object')\n",
      "Audio File: audio100.m4a -> Predicted Emotion: ['disappointment', 'fear', 'sadness'], Actual Emotion: sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio101.m4a:  The good bye was hard that I expected.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted Emotion for audio101.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio101.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio104.m4a:  This is probably the best action movie I've seen in a while. All the action sequences were incredible and well performed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio104.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio104.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio106.m4a:  I've never imagined that a simple walk through the city could bring so much reflection on life. The way each street and every corner reminds me of old memories and makes me question how quickly time passes by without truly noticing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted Emotion for audio106.m4a: Index(['realization'], dtype='object')\n",
      "Audio File: audio106.m4a -> Predicted Emotion: ['realization'], Actual Emotion: realization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio107.m4a:  I didn't think I would like this, but it completely surprised me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted Emotion for audio107.m4a: Index(['surprise'], dtype='object')\n",
      "Audio File: audio107.m4a -> Predicted Emotion: ['surprise'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio108.m4a:  Amazing, the plot wasn't expected, I thought I had to figure it out, but every twist kept me guessing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted Emotion for audio108.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio108.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio109.m4a:  Totally unexpected. This show took me by surprise in the best way possible.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio109.m4a: Index(['excitement', 'surprise'], dtype='object')\n",
      "Audio File: audio109.m4a -> Predicted Emotion: ['excitement', 'surprise'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio11.m4a:  I'm really impressed by the amount of work that went into this.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio11.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio11.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio110.m4a:  Totally caught me off guard.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio110.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio110.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio111.m4a:  That was way better than I expected.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted Emotion for audio111.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio111.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio112.m4a:  Joyful.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio112.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio112.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: joy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio113.m4a:  I cannot help but laugh. This experience has been pure joy from the start to finish.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Predicted Emotion for audio113.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio113.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: joy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio114.m4a:  This is such a nublifting experience, I feel light and joyful.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted Emotion for audio114.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio114.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: joy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio115.m4a:  That concert was pure joy, the energy from the crowd and the music had me dancing all night.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio115.m4a: Index(['admiration', 'excitement', 'joy', 'neutral'], dtype='object')\n",
      "Audio File: audio115.m4a -> Predicted Emotion: ['admiration', 'excitement', 'joy', 'neutral'], Actual Emotion: joy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio116.m4a:  Nothing ever seems to go right lately and it's getting exhausting.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio116.m4a: Index(['disappointment'], dtype='object')\n",
      "Audio File: audio116.m4a -> Predicted Emotion: ['disappointment'], Actual Emotion: sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio117.m4a:  I keep trying but nothing ever changes. It's all just the same.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio117.m4a: Index(['approval', 'optimism', 'neutral'], dtype='object')\n",
      "Audio File: audio117.m4a -> Predicted Emotion: ['approval', 'optimism', 'neutral'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio118.m4a:  I don't know how much longer I can keep going like this.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio118.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio118.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio119.m4a:  It feels like I'm stuck in a loop, unable to escape.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted Emotion for audio119.m4a: Index(['approval', 'neutral'], dtype='object')\n",
      "Audio File: audio119.m4a -> Predicted Emotion: ['approval', 'neutral'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio12.m4a:  That was truly amazing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio12.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio12.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio120.m4a:  The more I try the worse it gets.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Predicted Emotion for audio120.m4a: Index(['annoyance', 'disgust'], dtype='object')\n",
      "Audio File: audio120.m4a -> Predicted Emotion: ['annoyance', 'disgust'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio121.m4a:  It's impossible to keep up with everything.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted Emotion for audio121.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio121.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio122.m4a:  I'm getting tired of repeating myself.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio122.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio122.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio123.m4a:  I keep hearing dead ends no matter what I try.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio123.m4a: Index(['disapproval', 'neutral'], dtype='object')\n",
      "Audio File: audio123.m4a -> Predicted Emotion: ['disapproval', 'neutral'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio124.m4a:  Your support during this difficult time has been my anchor. I can't express how much that meant to me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted Emotion for audio124.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio124.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: gratitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio125.m4a:  The way you handled that challenging situation showed incredibly leadership and composure.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted Emotion for audio125.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio125.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio126.m4a:  Every time I think about the presentation tomorrow, my heart starts racing and I feel completely overwhelmed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Predicted Emotion for audio126.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio126.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio127.m4a:  Despite the setbacks, I believe we are on the right track and I will achieve our goal soon.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio127.m4a: Index(['admiration', 'approval'], dtype='object')\n",
      "Audio File: audio127.m4a -> Predicted Emotion: ['admiration', 'approval'], Actual Emotion: optimism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio128.m4a:  I spent weeks preparing for this opportunity only to have it cancel at the last minute.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted Emotion for audio128.m4a: Index(['realization', 'neutral'], dtype='object')\n",
      "Audio File: audio128.m4a -> Predicted Emotion: ['realization', 'neutral'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio129.m4a:  Finally, submitting that project felt like putting down a heavy backpack out been carried for months.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted Emotion for audio129.m4a: Index(['approval'], dtype='object')\n",
      "Audio File: audio129.m4a -> Predicted Emotion: ['approval'], Actual Emotion: relief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio13.m4a:  Incredible.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Predicted Emotion for audio13.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio13.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio130.m4a:  These shrugs have kept changing and now I'm not sure what I'm supposed to be focusing on.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted Emotion for audio130.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio130.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio131.m4a:  Just got the email confirming our vacation plans. I can wait to finally see the ocean.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Predicted Emotion for audio131.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio131.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio132.m4a:  When my phone started ringing during the speech, I wanted to disappear into the floor.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio132.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio132.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: embarrassment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio133.m4a:  I never expect them to remember my birthday and alone playing this entire celebration.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio133.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio133.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio134.m4a:  It's been a year since we lost him, but some days the pain feels just as fresh as the first day.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Predicted Emotion for audio134.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio134.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: grief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio135.m4a:  The dog's reaction to seeing snow for the first time had as laughing until we cried.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio135.m4a: Index(['amusement', 'sadness'], dtype='object')\n",
      "Audio File: audio135.m4a -> Predicted Emotion: ['amusement', 'sadness'], Actual Emotion: amusement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio136.m4a:  The state of the kitchen after my roommates party was absolutely revolting.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio136.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio136.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio137.m4a:  I've always wondered how they create those incredible special effects in movies.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio137.m4a: Index(['admiration', 'surprise'], dtype='object')\n",
      "Audio File: audio137.m4a -> Predicted Emotion: ['admiration', 'surprise'], Actual Emotion: curiosity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio138.m4a:  My hands won't stop shaking and I keep forgetting what I was about to say.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted Emotion for audio138.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio138.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: nervousness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio139.m4a:  After reviewing all the proposals, yours stands out as the most comprehensive and well-researched.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio139.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio139.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio14.m4a:  I had no idea the product would be this good. I'm honestly amazed at how it works.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted Emotion for audio14.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio14.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio140.m4a:  Looking through these old photographs brings back memories that make my heart ache.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio140.m4a: Index(['sadness'], dtype='object')\n",
      "Audio File: audio140.m4a -> Predicted Emotion: ['sadness'], Actual Emotion: sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio141.m4a:  Dancing in the rain with my best friends today was the most freeing feeling I've had in years.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio141.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio141.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: joy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio142.m4a:  If I could go back and change how I responded to her, I absolutely would.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio142.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio142.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: remorse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio143.m4a:  Watching you with our child makes my heart feel so full it might burst.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio143.m4a: Index(['sadness', 'neutral'], dtype='object')\n",
      "Audio File: audio143.m4a -> Predicted Emotion: ['sadness', 'neutral'], Actual Emotion: love\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio144.m4a:  They promised to fix this issue three weeks ago and nothing has been done.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio144.m4a: Index(['approval', 'optimism', 'neutral'], dtype='object')\n",
      "Audio File: audio144.m4a -> Predicted Emotion: ['approval', 'optimism', 'neutral'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio145.m4a:  It just click that the solution was hitting in plain sight all along.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio145.m4a: Index(['annoyance'], dtype='object')\n",
      "Audio File: audio145.m4a -> Predicted Emotion: ['annoyance'], Actual Emotion: realization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio146.m4a:  This approach completely disregards the feedback we received from the community.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted Emotion for audio146.m4a: Index(['approval', 'neutral'], dtype='object')\n",
      "Audio File: audio146.m4a -> Predicted Emotion: ['approval', 'neutral'], Actual Emotion: disapproval \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio147.m4a:  I made your favorite soup since I've heard you weren't feeling well.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Predicted Emotion for audio147.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio147.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: caring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio148.m4a:  Could you please stop taping your pen? It's making it impossible to concentrate.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio148.m4a: Index(['approval'], dtype='object')\n",
      "Audio File: audio148.m4a -> Predicted Emotion: ['approval'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio149.m4a:  Seeing my daughter graduate today after everything she's overcome, I couldn't be more proud.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio149.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio149.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: pride\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio15.m4a:  The way the show took such an enormous twist made me laugh so loud. I hadn't expect this to be this entertaining.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio15.m4a: Index(['admiration', 'amusement', 'surprise'], dtype='object')\n",
      "Audio File: audio15.m4a -> Predicted Emotion: ['admiration', 'amusement', 'surprise'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio150.m4a:  Wait, so the meeting is cancelled? But I just received a reminder about it 5 minutes ago.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Predicted Emotion for audio150.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio150.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio151.m4a:  I've had my high on that vintage record player for months. I think I'm finally going to treat myself.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Predicted Emotion for audio151.m4a: Index(['admiration', 'love'], dtype='object')\n",
      "Audio File: audio151.m4a -> Predicted Emotion: ['admiration', 'love'], Actual Emotion: desire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio152.m4a:  The strange noises coming from outside my window made it impossible to fall asleep last night.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Predicted Emotion for audio152.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio152.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio153.m4a:  We just got approved for our dream house. I still can't believe it's really happening.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Predicted Emotion for audio153.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio153.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio154.m4a:  The film had such potential, but completely fell apart in the second half.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step\n",
      "Predicted Emotion for audio154.m4a: Index(['approval', 'disapproval'], dtype='object')\n",
      "Audio File: audio154.m4a -> Predicted Emotion: ['approval', 'disapproval'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio155.m4a:  I hope the box is expecting the usual gift card, but found tickets to my favorite band instead.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "Predicted Emotion for audio155.m4a: Index(['admiration', 'excitement', 'optimism'], dtype='object')\n",
      "Audio File: audio155.m4a -> Predicted Emotion: ['admiration', 'excitement', 'optimism'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio156.m4a:  The smell in the refrigerator was so bad I had to step outside to catch my breath.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
      "Predicted Emotion for audio156.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio156.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio157.m4a:  Your ability to remain calm and their pressure is something I've always aspired to develop.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Predicted Emotion for audio157.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio157.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio158.m4a:  I've been rehearsing this speech for days, but now my mind is completely blank.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "Predicted Emotion for audio158.m4a: Index(['approval', 'neutral'], dtype='object')\n",
      "Audio File: audio158.m4a -> Predicted Emotion: ['approval', 'neutral'], Actual Emotion: nervousness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio159.m4a:  The way you stepped into help without being asked to showed real kindness and generosity.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "Predicted Emotion for audio159.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio159.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: gratitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio16.m4a:  The movie was hilarious from start to finish, I laughed more than I expected.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "Predicted Emotion for audio16.m4a: Index(['amusement', 'neutral'], dtype='object')\n",
      "Audio File: audio16.m4a -> Predicted Emotion: ['amusement', 'neutral'], Actual Emotion: amusement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio160.m4a:  Driving past our old neighborhood brought back a float of memories I wasn't prepared for.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "Predicted Emotion for audio160.m4a: Index(['disappointment', 'sadness'], dtype='object')\n",
      "Audio File: audio160.m4a -> Predicted Emotion: ['disappointment', 'sadness'], Actual Emotion: sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio161.m4a:  When the dog we thought was lost forever, came running back. Poor Joy doesn't begin to describe it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step\n",
      "Predicted Emotion for audio161.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio161.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: joy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio162.m4a:  I should have listened to your advice instead of being so stubborn about doing it my way.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "Predicted Emotion for audio162.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio162.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: remorse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio163.m4a:  How exactly do they train dolphins to perform such complex routines? It's fascinating.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "Predicted Emotion for audio163.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio163.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: curiosity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio164.m4a:  After waiting for two hours, they told us our reservation and been misplaced.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Predicted Emotion for audio164.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio164.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio165.m4a:  The committee anonymously supported your recommendation. It was clearly the right call.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio165.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio165.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio166.m4a:  I called him by the wrong name twice during our entire conversation.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio166.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio166.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: embarrassment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio167.m4a:  Being surrounded by family during the holidays reminds me how fortunate I am to have these bonds.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio167.m4a: Index(['joy'], dtype='object')\n",
      "Audio File: audio167.m4a -> Predicted Emotion: ['joy'], Actual Emotion: love\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio168.m4a:  I really thought this time the outcome would be different, but here we are again.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted Emotion for audio168.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio168.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio169.m4a:  When the test results came back negative, I could finally breathe again.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio169.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio169.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: relief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio17.m4a:  I couldn't stop laughing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio17.m4a: Index(['amusement', 'anger', 'annoyance'], dtype='object')\n",
      "Audio File: audio17.m4a -> Predicted Emotion: ['amusement', 'anger', 'annoyance'], Actual Emotion: amusement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio170.m4a:  I can't believe someone would live that trash all over this beautiful hiking trail.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted Emotion for audio170.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio170.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio171.m4a:  Completing the marathon wasn't about the time, it was about proving to myself I could do it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted Emotion for audio171.m4a: Index(['optimism', 'neutral'], dtype='object')\n",
      "Audio File: audio171.m4a -> Predicted Emotion: ['optimism', 'neutral'], Actual Emotion: pride\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio172.m4a:  So we are supposed to submit the form online, but the website says in-person submissions only.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio172.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio172.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio173.m4a:  I never expected the small cafe around the corner to serve the best meal I've had all year.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted Emotion for audio173.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio173.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio174.m4a:  The turbulence on that flight was so severe I was convinced we wouldn't make it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio174.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio174.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio175.m4a:  The auto correct fail in that important email had the entire office in stitches.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted Emotion for audio175.m4a: Index(['approval', 'neutral'], dtype='object')\n",
      "Audio File: audio175.m4a -> Predicted Emotion: ['approval', 'neutral'], Actual Emotion: amusement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio176.m4a:  It's the little things that remind me they are gone, like seeing their favorite serial in the store.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio176.m4a: Index(['admiration', 'approval'], dtype='object')\n",
      "Audio File: audio176.m4a -> Predicted Emotion: ['admiration', 'approval'], Actual Emotion: grief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio177.m4a:  Just one more chapter before bed. This book is too good to put down.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predicted Emotion for audio177.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio177.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: desire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio178.m4a:  Could everyone please stop interrupting? I haven't been able to finish a single thought.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio178.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio178.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio179.m4a:  I noticed you've been working late, so I brought you some dinner from that place you like.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio179.m4a: Index(['realization'], dtype='object')\n",
      "Audio File: audio179.m4a -> Predicted Emotion: ['realization'], Actual Emotion: caring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio18.m4a:  It was too funny.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted Emotion for audio18.m4a: Index(['amusement', 'joy'], dtype='object')\n",
      "Audio File: audio18.m4a -> Predicted Emotion: ['amusement', 'joy'], Actual Emotion: amusement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio180.m4a:  This policy change completely undermines everything we've been working towards.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted Emotion for audio180.m4a: Index(['approval', 'disapproval', 'neutral'], dtype='object')\n",
      "Audio File: audio180.m4a -> Predicted Emotion: ['approval', 'disapproval', 'neutral'], Actual Emotion: disapproval \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio181.m4a:  I've been approaching this problem from the wrong angle. No wonder nothing was working.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted Emotion for audio181.m4a: Index(['surprise'], dtype='object')\n",
      "Audio File: audio181.m4a -> Predicted Emotion: ['surprise'], Actual Emotion: realization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio182.m4a:  My stomach is not waiting for them to announce the results.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted Emotion for audio182.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio182.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: nervousness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio183.m4a:  The forecast says rain all weekend, but I'm still hopeful we'll get some sunshine for the event.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio183.m4a: Index(['optimism'], dtype='object')\n",
      "Audio File: audio183.m4a -> Predicted Emotion: ['optimism'], Actual Emotion: optimism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio184.m4a:  Putting away their toys, knowing they've outgrown them, hit me harder than I expected.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio184.m4a: Index(['annoyance'], dtype='object')\n",
      "Audio File: audio184.m4a -> Predicted Emotion: ['annoyance'], Actual Emotion: sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio185.m4a:  The level of detail in this painting is extraordinary. I can't stop noticing new elements.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio185.m4a: Index(['annoyance'], dtype='object')\n",
      "Audio File: audio185.m4a -> Predicted Emotion: ['annoyance'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio186.m4a:  I wish I had taken the time to visit more often before it was too late.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio186.m4a: Index(['desire', 'optimism'], dtype='object')\n",
      "Audio File: audio186.m4a -> Predicted Emotion: ['desire', 'optimism'], Actual Emotion: remorse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio187.m4a:  Finding your umbrella in my bag when the storm hit reminded me how thoughtful you always are.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted Emotion for audio187.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio187.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: gratitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio188.m4a:  The power went out during the storm and every sound in the darkness seemed threatening.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio188.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio188.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio189.m4a:  Watching the sunrise from the mountain top after hours of hiking, absolutely worth every step.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted Emotion for audio189.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio189.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: joy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio19.m4a:  The comedian's timing was perfect and every punchline it just right.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted Emotion for audio19.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio19.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: amusement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio190.m4a:  They made these decisions without consulting anyone who would be affected by them.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio190.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio190.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio191.m4a:  We just booked our tickets for that festival we've been talking about for years.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio191.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio191.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio192.m4a:  The reality of the experience didn't come close to matching the hype surrounding it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted Emotion for audio192.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio192.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio193.m4a:  I turned the corner and there they all were waiting to surprise me. I was completely speechless.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio193.m4a: Index(['surprise'], dtype='object')\n",
      "Audio File: audio193.m4a -> Predicted Emotion: ['surprise'], Actual Emotion: surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio194.m4a:  The way they spoke about those vulnerable people showed a complete lack of empathy.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted Emotion for audio194.m4a: Index(['annoyance', 'neutral'], dtype='object')\n",
      "Audio File: audio194.m4a -> Predicted Emotion: ['annoyance', 'neutral'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio195.m4a:  This is exactly the direction we should be taking. It addresses all our key concerns.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted Emotion for audio195.m4a: Index(['approval', 'neutral'], dtype='object')\n",
      "Audio File: audio195.m4a -> Predicted Emotion: ['approval', 'neutral'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio196.m4a:  Wait, so the deadline was yesterday, but the email clearly said next week.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted Emotion for audio196.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio196.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio197.m4a:  I've always been fascinated by how certain songs can instantly transport you back in time.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio197.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio197.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: curiosity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio198.m4a:  I realized halfway through my story that I was confusing them with someone else entirely.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted Emotion for audio198.m4a: Index(['realization'], dtype='object')\n",
      "Audio File: audio198.m4a -> Predicted Emotion: ['realization'], Actual Emotion: embarrassment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio199.m4a:  When they call to say they'd found my lost wallet with everything still inside, pure relief.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio199.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio199.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: relief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio2.m4a:  I cannot believe this is happening again. It is so frustrating.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted Emotion for audio2.m4a: Index(['anger', 'annoyance', 'disapproval'], dtype='object')\n",
      "Audio File: audio2.m4a -> Predicted Emotion: ['anger', 'annoyance', 'disapproval'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio20.m4a:  I couldn't believe how the situation unfolded. The complete lack of respect was infuriating and it took everything I had to stay calm.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio20.m4a: Index(['anger', 'annoyance'], dtype='object')\n",
      "Audio File: audio20.m4a -> Predicted Emotion: ['anger', 'annoyance'], Actual Emotion: amusement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio200.m4a:  The constant notifications from this app are driving me crazy.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted Emotion for audio200.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio200.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio201.m4a:  Watching you pursue your passion with such dedication makes me fall in love with you all over again.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio201.m4a: Index(['admiration', 'love'], dtype='object')\n",
      "Audio File: audio201.m4a -> Predicted Emotion: ['admiration', 'love'], Actual Emotion: love\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio202.m4a:  Some days I still reach for the phone to call them before remembering they are gone.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio202.m4a: Index(['love'], dtype='object')\n",
      "Audio File: audio202.m4a -> Predicted Emotion: ['love'], Actual Emotion: grief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio203.m4a:  Seeing my name published alongside researchers I've admired for years, still can't believe it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio203.m4a: Index(['surprise'], dtype='object')\n",
      "Audio File: audio203.m4a -> Predicted Emotion: ['surprise'], Actual Emotion: pride\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio204.m4a:  I had always respected the project from afar, but seeing the final version in person made me truly admire the dedication that went into it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predicted Emotion for audio204.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio204.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: admiration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio205.m4a:  The absurdity of the situation was so unexpected that I couldn't help but laugh out loud, completely caught me off guard.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted Emotion for audio205.m4a: Index(['amusement'], dtype='object')\n",
      "Audio File: audio205.m4a -> Predicted Emotion: ['amusement'], Actual Emotion: amusement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio206.m4a:  When I saw how they brushed off my concerns without even a second thought, my frustration reached a boiling point.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted Emotion for audio206.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio206.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio207.m4a:  I was initially skeptical, but now I can confidently say that the outcome is exactly what we needed. Well done.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted Emotion for audio207.m4a: Index(['admiration', 'approval'], dtype='object')\n",
      "Audio File: audio207.m4a -> Predicted Emotion: ['admiration', 'approval'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio208.m4a:  The more I tried to figure out where it was going on, the more lost I became in the complexity of it all.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio208.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio208.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio209.m4a:  I had high expectations, but the outcome was underwhelming in every way. It's hard to feel anything but let down.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio209.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio209.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio21.m4a:  It was hard to keep my composure when I saw how they handled that, completely unacceptable.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio21.m4a: Index(['disapproval', 'neutral'], dtype='object')\n",
      "Audio File: audio21.m4a -> Predicted Emotion: ['disapproval', 'neutral'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio22.m4a:  So frustrating.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predicted Emotion for audio22.m4a: Index(['anger', 'annoyance'], dtype='object')\n",
      "Audio File: audio22.m4a -> Predicted Emotion: ['anger', 'annoyance'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio23.m4a:  I cannot believe this is happening.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio23.m4a: Index(['disapproval'], dtype='object')\n",
      "Audio File: audio23.m4a -> Predicted Emotion: ['disapproval'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio24.m4a:  The way they ignored the issue only made it worse. I was fuming the whole time.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio24.m4a: Index(['annoyance', 'disgust'], dtype='object')\n",
      "Audio File: audio24.m4a -> Predicted Emotion: ['annoyance', 'disgust'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio25.m4a:  I was tried to stay patient but the constant interruptions are really starting to get on my nerves. It's hard to focus at all.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio25.m4a: Index(['fear'], dtype='object')\n",
      "Audio File: audio25.m4a -> Predicted Emotion: ['fear'], Actual Emotion: anger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio26.m4a:  The repeated delays are becoming unbearable. How many times do we have to go over the same thing?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted Emotion for audio26.m4a: Index(['anger', 'annoyance'], dtype='object')\n",
      "Audio File: audio26.m4a -> Predicted Emotion: ['anger', 'annoyance'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio27.m4a:  Unbelievable.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio27.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio27.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio28.m4a:  This was super annoying.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted Emotion for audio28.m4a: Index(['anger', 'annoyance'], dtype='object')\n",
      "Audio File: audio28.m4a -> Predicted Emotion: ['anger', 'annoyance'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio29.m4a:  It's frustrating when things don't go as planned and it's been one setback after another today.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio29.m4a: Index(['anger', 'annoyance', 'neutral'], dtype='object')\n",
      "Audio File: audio29.m4a -> Predicted Emotion: ['anger', 'annoyance', 'neutral'], Actual Emotion: annoyance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio3.m4a:  I really don't think this is the right decision.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted Emotion for audio3.m4a: Index(['disapproval', 'neutral'], dtype='object')\n",
      "Audio File: audio3.m4a -> Predicted Emotion: ['disapproval', 'neutral'], Actual Emotion: disapproval \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio30.m4a:  I didn't expect this to work out, but I'm glad to see its exceeded expectations.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio30.m4a: Index(['admiration', 'joy'], dtype='object')\n",
      "Audio File: audio30.m4a -> Predicted Emotion: ['admiration', 'joy'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio31.m4a:  and fully behind this great job\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted Emotion for audio31.m4a: Index(['admiration', 'gratitude'], dtype='object')\n",
      "Audio File: audio31.m4a -> Predicted Emotion: ['admiration', 'gratitude'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio32.m4a:  After carefully considering everything, I can comfortably say I approve the way this project is shaping up. It's exactly what we needed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predicted Emotion for audio32.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio32.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio33.m4a:  This looks great.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Predicted Emotion for audio33.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio33.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio34.m4a:  Everything checks out and I'm fully on board with how things are processing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio34.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio34.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: approval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio35.m4a:  It means a lot to me that you took the time to listen and offer support, especially when I wasn't sure what I needed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted Emotion for audio35.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio35.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: caring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio36.m4a:  That was so thoughtful of you.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted Emotion for audio36.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio36.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: caring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio37.m4a:  Thank you.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio37.m4a: Index(['gratitude'], dtype='object')\n",
      "Audio File: audio37.m4a -> Predicted Emotion: ['gratitude'], Actual Emotion: caring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio38.m4a:  The small gestures often show the most care and I really appreciate you looking out for me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted Emotion for audio38.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio38.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: caring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio39.m4a:  I thought I had everything under control, but as I got deeper into the situation, it became increasingly clear that I was completely out of my depth.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted Emotion for audio39.m4a: Index(['approval', 'realization'], dtype='object')\n",
      "Audio File: audio39.m4a -> Predicted Emotion: ['approval', 'realization'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio4.m4a:  Thank you so much for your help. I truly appreciate it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted Emotion for audio4.m4a: Index(['admiration', 'gratitude'], dtype='object')\n",
      "Audio File: audio4.m4a -> Predicted Emotion: ['admiration', 'gratitude'], Actual Emotion: gratitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio40.m4a:  I'm not sure what's happening here.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted Emotion for audio40.m4a: Index(['confusion', 'disapproval', 'neutral'], dtype='object')\n",
      "Audio File: audio40.m4a -> Predicted Emotion: ['confusion', 'disapproval', 'neutral'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio41.m4a:  Not entirely sure why Whisper doesn't work on Google Collab.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted Emotion for audio41.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio41.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio42.m4a:  The instructions were so unclear I couldn't tell if I was doing the right thing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio42.m4a: Index(['disappointment'], dtype='object')\n",
      "Audio File: audio42.m4a -> Predicted Emotion: ['disappointment'], Actual Emotion: confusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio43.m4a:  I couldn't help wonder how things would play out if we took a different approach. I'm really curious to see where this path leads.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio43.m4a: Index(['curiosity', 'surprise'], dtype='object')\n",
      "Audio File: audio43.m4a -> Predicted Emotion: ['curiosity', 'surprise'], Actual Emotion: curiosity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio44.m4a:  I need to know more about this.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted Emotion for audio44.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio44.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: curiosity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio45.m4a:  I'm dying to know how they pulled this off.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted Emotion for audio45.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio45.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: curiosity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio46.m4a:  There's something that doesn't add up and I'm curious to find out the truth.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted Emotion for audio46.m4a: Index(['confusion', 'curiosity', 'neutral'], dtype='object')\n",
      "Audio File: audio46.m4a -> Predicted Emotion: ['confusion', 'curiosity', 'neutral'], Actual Emotion: curiosity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio47.m4a:  I've been thinking about trying the new restaurant for weeks. After hearing such great things I can wait to see if it lips up to the hype.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted Emotion for audio47.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio47.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: desire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio48.m4a:  I've been craving quite we can get away for so long and other it's finally happening I can wait to escape.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Predicted Emotion for audio48.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio48.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: desire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio49.m4a:  I really want to see that movie.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio49.m4a: Index(['desire', 'optimism'], dtype='object')\n",
      "Audio File: audio49.m4a -> Predicted Emotion: ['desire', 'optimism'], Actual Emotion: desire\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio5.m4a:  I wonder how that works. I'd love to know more about it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Predicted Emotion for audio5.m4a: Index(['excitement', 'surprise'], dtype='object')\n",
      "Audio File: audio5.m4a -> Predicted Emotion: ['excitement', 'surprise'], Actual Emotion: curiosity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio50.m4a:  That movie was such a light down.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio50.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio50.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio51.m4a:  I'm bummed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio51.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio51.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio52.m4a:  I was really looking forward to this event, but when I arrived and I saw how poorly organized it was, I couldn't help but feel let down.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio52.m4a: Index(['sadness'], dtype='object')\n",
      "Audio File: audio52.m4a -> Predicted Emotion: ['sadness'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio53.m4a:  I expected better from this product, but it didn't quite meet my expectations, and I'm glad to feeling disappointed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio53.m4a: Index(['disappointment', 'sadness'], dtype='object')\n",
      "Audio File: audio53.m4a -> Predicted Emotion: ['disappointment', 'sadness'], Actual Emotion: disappointment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio54.m4a:  That movie was terrible, just awful.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted Emotion for audio54.m4a: Index(['disgust', 'fear'], dtype='object')\n",
      "Audio File: audio54.m4a -> Predicted Emotion: ['disgust', 'fear'], Actual Emotion: disapproval \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio55.m4a:  Not ideal.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted Emotion for audio55.m4a: Index(['disapproval', 'neutral'], dtype='object')\n",
      "Audio File: audio55.m4a -> Predicted Emotion: ['disapproval', 'neutral'], Actual Emotion: disapproval \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio56.m4a:  I don't think this is the right choice.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted Emotion for audio56.m4a: Index(['disapproval', 'neutral'], dtype='object')\n",
      "Audio File: audio56.m4a -> Predicted Emotion: ['disapproval', 'neutral'], Actual Emotion: disapproval \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio57.m4a:  I wasn't impressed with how this was handled. It doesn't lie with the standards I expect.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio57.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio57.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: disapproval \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio58.m4a:  The presentation was so poorly organized that it was hard to focus on anything. Nothing about it felt professional or well thought out.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio58.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio58.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio59.m4a:  Honestly, I didn't expect this to be so bad. The whole thing left a bad taste in my mouth.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio59.m4a: Index(['disappointment'], dtype='object')\n",
      "Audio File: audio59.m4a -> Predicted Emotion: ['disappointment'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio60.m4a:  That was hard to watch.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio60.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio60.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio61.m4a:  So of putting.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio61.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio61.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio62.m4a:  There was no way I can get past how poorly this turned out.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted Emotion for audio62.m4a: Index(['disapproval'], dtype='object')\n",
      "Audio File: audio62.m4a -> Predicted Emotion: ['disapproval'], Actual Emotion: disgust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio63.m4a:  When I trip in front of everyone my face turned red. Sometimes all you can do is life head off.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Predicted Emotion for audio63.m4a: Index(['approval', 'neutral'], dtype='object')\n",
      "Audio File: audio63.m4a -> Predicted Emotion: ['approval', 'neutral'], Actual Emotion: embarrassment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio64.m4a:  That was so awkward.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio64.m4a: Index(['annoyance'], dtype='object')\n",
      "Audio File: audio64.m4a -> Predicted Emotion: ['annoyance'], Actual Emotion: embarrassment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio65.m4a:  It's hard to recover from that kind of slip up, but I did my best to move on quickly.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio65.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio65.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: embarrassment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio66.m4a:  I tried to keep my composure, but when I was called out I couldn't help a filled mortified.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Predicted Emotion for audio66.m4a: Index(['sadness'], dtype='object')\n",
      "Audio File: audio66.m4a -> Predicted Emotion: ['sadness'], Actual Emotion: embarrassment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio67.m4a:  When I walked into the concert and I felt the energy in the crowd, I knew this is going to be an unforgettable experience.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Predicted Emotion for audio67.m4a: Index(['admiration', 'approval'], dtype='object')\n",
      "Audio File: audio67.m4a -> Predicted Emotion: ['admiration', 'approval'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio68.m4a:  The game was packed with surprises and the adrenaline kept me glued to the screen for hours.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio68.m4a: Index(['surprise'], dtype='object')\n",
      "Audio File: audio68.m4a -> Predicted Emotion: ['surprise'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio69.m4a:  The new software update would smooth and fast. I'm really pumped about this improvements.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio69.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio69.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio7.m4a:  Oh, I get it now. That was why everything wasn't out of the up.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted Emotion for audio7.m4a: Index(['confusion', 'curiosity'], dtype='object')\n",
      "Audio File: audio7.m4a -> Predicted Emotion: ['confusion', 'curiosity'], Actual Emotion: realization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio70.m4a:  We just wrapped up an incredible trip and every moment was filled with so much energy and excitement.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted Emotion for audio70.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio70.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio71.m4a:  After hearing about this release for weeks, I finally saw it live.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted Emotion for audio71.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio71.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: excitement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio72.m4a:  The suspense was unbearable as we waited for the results to come in. With each passing moment I couldn't shake the sense of threat building inside me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted Emotion for audio72.m4a: Index(['fear'], dtype='object')\n",
      "Audio File: audio72.m4a -> Predicted Emotion: ['fear'], Actual Emotion: fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio73.m4a:  It wasn't just the unknown that scared me, it was the feeling that something could go or will be wrong at any moment.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted Emotion for audio73.m4a: Index(['fear'], dtype='object')\n",
      "Audio File: audio73.m4a -> Predicted Emotion: ['fear'], Actual Emotion: fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio74.m4a:  I'm not sure I can handle this.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted Emotion for audio74.m4a: Index(['approval', 'neutral'], dtype='object')\n",
      "Audio File: audio74.m4a -> Predicted Emotion: ['approval', 'neutral'], Actual Emotion: fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio75.m4a:  I'm terrified.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted Emotion for audio75.m4a: Index(['fear'], dtype='object')\n",
      "Audio File: audio75.m4a -> Predicted Emotion: ['fear'], Actual Emotion: fear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio76.m4a:  I appreciate this more than you know.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted Emotion for audio76.m4a: Index(['admiration', 'gratitude'], dtype='object')\n",
      "Audio File: audio76.m4a -> Predicted Emotion: ['admiration', 'gratitude'], Actual Emotion: gratitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio77.m4a:  I can thank you enough for the all the support that you've given me. It's hard to express how much it means that you're always there for me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted Emotion for audio77.m4a: Index(['gratitude'], dtype='object')\n",
      "Audio File: audio77.m4a -> Predicted Emotion: ['gratitude'], Actual Emotion: gratitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio78.m4a:  I'm so thankful for everything you've done for me. It really made a difference during the tough time.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted Emotion for audio78.m4a: Index(['gratitude'], dtype='object')\n",
      "Audio File: audio78.m4a -> Predicted Emotion: ['gratitude'], Actual Emotion: gratitude\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio79.m4a:  I've been trying to stay strong, but it's hard when grief seems to follow me everywhere.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted Emotion for audio79.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio79.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: grief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio8.m4a:  I'm sure things will get better soon. We've got this.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted Emotion for audio8.m4a: Index(['approval', 'optimism', 'neutral'], dtype='object')\n",
      "Audio File: audio8.m4a -> Predicted Emotion: ['approval', 'optimism', 'neutral'], Actual Emotion: optimism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio80.m4a:  Even in the busiest moments there's a lingering feeling of loss that I can't shake off.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted Emotion for audio80.m4a: Index(['disappointment'], dtype='object')\n",
      "Audio File: audio80.m4a -> Predicted Emotion: ['disappointment'], Actual Emotion: grief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio81.m4a:  I can begin to describe how much this means to me. It's more than just a failing. It's something deep inside that can't put into words.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Emotion for audio81.m4a: Index(['annoyance'], dtype='object')\n",
      "Audio File: audio81.m4a -> Predicted Emotion: ['annoyance'], Actual Emotion: love\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio82.m4a:  Every little thing you do just makes me love you more.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted Emotion for audio82.m4a: Index(['love'], dtype='object')\n",
      "Audio File: audio82.m4a -> Predicted Emotion: ['love'], Actual Emotion: love\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio83.m4a:  I didn't expect to feel so loved, but every little gesture shows how much you care.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted Emotion for audio83.m4a: Index(['admiration'], dtype='object')\n",
      "Audio File: audio83.m4a -> Predicted Emotion: ['admiration'], Actual Emotion: love\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio84.m4a:  I could feel my palm sweating as I prepared for the meeting and despite the preparation the nerves were still there racing through my mind.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Predicted Emotion for audio84.m4a: Index(['optimism'], dtype='object')\n",
      "Audio File: audio84.m4a -> Predicted Emotion: ['optimism'], Actual Emotion: nervousness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio85.m4a:  The uncertainty of it all left me feeling on edge.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "Predicted Emotion for audio85.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio85.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: nervousness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio86.m4a:  After everything we've been through, I can help but feel proud of what we've accomplished together.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "Predicted Emotion for audio86.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio86.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: pride\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio87.m4a:  Looking at how much we've achieved, I feel an overwhelming sense of pride in what we've done.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Predicted Emotion for audio87.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio87.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: pride\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio88.m4a:  We did it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "Predicted Emotion for audio88.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio88.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: pride\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio89.m4a:  I can help but smile when I think about what was achieved.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Predicted Emotion for audio89.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio89.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: pride\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio9.m4a:  It's been a rocky start, but I have a strong feeling that things are about to take off.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Predicted Emotion for audio9.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio9.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: optimism\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio90.m4a:  As the PC started to fall into place, I had a sudden realization that what I've been searching for was right in front of me for the whole time.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Predicted Emotion for audio90.m4a: Index(['realization', 'neutral'], dtype='object')\n",
      "Audio File: audio90.m4a -> Predicted Emotion: ['realization', 'neutral'], Actual Emotion: realization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio91.m4a:  Oh, I get it now.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "Predicted Emotion for audio91.m4a: Index(['surprise'], dtype='object')\n",
      "Audio File: audio91.m4a -> Predicted Emotion: ['surprise'], Actual Emotion: realization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio92.m4a:  It didn't hit me until that moment, but I realized I've been missing the obvious solution all along.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Predicted Emotion for audio92.m4a: Index(['realization'], dtype='object')\n",
      "Audio File: audio92.m4a -> Predicted Emotion: ['realization'], Actual Emotion: realization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio93.m4a:  After all the uncertainty and waiting, it was such a relief to finally see everything falling to place and no it was all gonna be okay.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "Predicted Emotion for audio93.m4a: Index(['joy'], dtype='object')\n",
      "Audio File: audio93.m4a -> Predicted Emotion: ['joy'], Actual Emotion: relief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio94.m4a:  When I heard the good news, it was like a way to have been lived off my shoulders.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "Predicted Emotion for audio94.m4a: Index(['admiration', 'neutral'], dtype='object')\n",
      "Audio File: audio94.m4a -> Predicted Emotion: ['admiration', 'neutral'], Actual Emotion: relief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio95.m4a:  Looking back, I realized I should have handled things differently and the weight of my decision continues to weight heavily on me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n",
      "Predicted Emotion for audio95.m4a: Index(['optimism', 'realization'], dtype='object')\n",
      "Audio File: audio95.m4a -> Predicted Emotion: ['optimism', 'realization'], Actual Emotion: remorse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio96.m4a:  I've learned a valuable lesson, though the remorse still hangs over me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "Predicted Emotion for audio96.m4a: Index(['realization'], dtype='object')\n",
      "Audio File: audio96.m4a -> Predicted Emotion: ['realization'], Actual Emotion: remorse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio97.m4a:  If only I had taken a moment to think it through, maybe it wouldn't feel this way right now.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Predicted Emotion for audio97.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio97.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: remorse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio98.m4a:  As I walked through the empty house, the realization that it was all over hit me hard. The silence was felt so heavy.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Predicted Emotion for audio98.m4a: Index(['neutral'], dtype='object')\n",
      "Audio File: audio98.m4a -> Predicted Emotion: ['neutral'], Actual Emotion: sadness\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription for audio99.m4a:  It's hard to put into words but seeing everything change felt like a dip sadness that I can quite to explain.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Predicted Emotion for audio99.m4a: Index(['disappointment', 'sadness'], dtype='object')\n",
      "Audio File: audio99.m4a -> Predicted Emotion: ['disappointment', 'sadness'], Actual Emotion: sadness\n",
      "Error in accuracy calculation: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.\n"
     ]
    }
   ],
   "source": [
    "# Loop through all audio files in the folder and process them\n",
    "for audio_file in os.listdir(audio_folder_path):\n",
    "    if audio_file.endswith(('.mp4','.mp3', '.m4a', '.wav', '.flac')):  # Check for relevant audio file extensions\n",
    "        audio_path = os.path.join(audio_folder_path, audio_file)\n",
    "        transcribed_text, predicted_emotion, actual_emotion = process_audio(audio_path, audio_file)\n",
    "\n",
    "        if transcribed_text and predicted_emotion and actual_emotion:\n",
    "            print(f\"Audio File: {audio_file} -> Predicted Emotion: {predicted_emotion}, Actual Emotion: {actual_emotion}\")\n",
    "            # Append the results for accuracy calculation\n",
    "            predicted_emotions.append(predicted_emotion)\n",
    "            actual_emotions.append(actual_emotion)\n",
    "\n",
    "# Ensure no missing values before calculating accuracy\n",
    "if predicted_emotions and actual_emotions:\n",
    "    try:\n",
    "        # Compute accuracy score\n",
    "        accuracy = accuracy_score(actual_emotions, predicted_emotions)\n",
    "        print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in accuracy calculation: {e}\")\n",
    "else:\n",
    "    print(\"Error: Missing predictions or actual emotions, cannot calculate accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (actual in predicted): 23.41%\n"
     ]
    }
   ],
   "source": [
    "# To get the accuracy\n",
    "\n",
    "correct = 0\n",
    "for pred, actual in zip(predicted_emotions, actual_emotions):\n",
    "    if actual in pred:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(actual_emotions)\n",
    "print(f\"Accuracy (actual in predicted): {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now run the model for the original script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: That was a fantastic presentation. I'm really impressed. \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Predicted: ['admiration'], Actual: approval\n",
      "\n",
      "Text: I cannot believe this is happening again. It is so frustrating.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['anger', 'annoyance', 'disapproval'], Actual: annoyance\n",
      "\n",
      "Text: I really don't think this is the right decision.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted: ['disapproval', 'neutral'], Actual: disapproval \n",
      "\n",
      "Text: Thank you so much for your help. I truly appreciate it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted: ['admiration', 'gratitude'], Actual: gratitude\n",
      "\n",
      "Text: I wonder how that works. I'd love to know more about it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Predicted: ['excitement', 'surprise'], Actual: curiosity\n",
      "\n",
      "Text: Oh, I get it now. That was why everything wasn't adding up.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['curiosity', 'neutral'], Actual: realization\n",
      "\n",
      "Text: I'm sure things will get better soon. We've got this.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted: ['approval', 'optimism', 'neutral'], Actual: optimism\n",
      "\n",
      "Text: It's been a rocky start, but I have a strong feeling that things are about to take off.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['admiration', 'neutral'], Actual: optimism\n",
      "\n",
      "Text: After watching the performance, I couldn't help but admire the level of skill and dedication.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted: ['admiration', 'neutral'], Actual: admiration\n",
      "\n",
      "Text: I’m really impressed by the amount of work that went into this\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['admiration'], Actual: admiration\n",
      "\n",
      "Text: That was truly amazing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Predicted: ['admiration'], Actual: admiration\n",
      "\n",
      "Text: Incredible!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['admiration'], Actual: admiration\n",
      "\n",
      "Text: I had no idea the product would be this good. I'm honestly amazed at how well it works.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['admiration', 'approval'], Actual: admiration\n",
      "\n",
      "Text: The way the show took such an enormous twist made me laugh so loud. I hadn't expect this to be this entertaining\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['admiration', 'amusement', 'surprise'], Actual: admiration\n",
      "\n",
      "Text: The movie was hilarious from start to finish. I laughed more than I expected.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['amusement', 'neutral'], Actual: amusement\n",
      "\n",
      "Text: I couldn't stop laughing\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['amusement', 'anger', 'annoyance'], Actual: amusement\n",
      "\n",
      "Text: It was too funny.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['amusement', 'joy'], Actual: amusement\n",
      "\n",
      "Text: The comedian's timing was perfect and every punchline hit just right.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted: ['admiration', 'neutral'], Actual: amusement\n",
      "\n",
      "Text: I couldn't believe how the situation unfolded. The complete lack of respect was infuriating and it took everything I had to stay calm.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['anger', 'annoyance'], Actual: amusement\n",
      "\n",
      "Text: It was hard to keep my composure when I saw how they handled that—completely unacceptable\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['neutral'], Actual: anger\n",
      "\n",
      "Text: So frustrating!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['anger', 'annoyance'], Actual: anger\n",
      "\n",
      "Text: I cannot believe this is happening!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['disapproval'], Actual: anger\n",
      "\n",
      "Text: The way they ignored the issue only made it worse. I was fuming the whole time.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['annoyance', 'disgust'], Actual: anger\n",
      "\n",
      "Text: I was trying to stay patient but the constant interruptions are really starting to get on my nerves. It's hard to focus at all.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted: ['fear'], Actual: anger\n",
      "\n",
      "Text: The repeated delays are becoming unbearable. How many times do we have to go over the same thing?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['anger', 'annoyance'], Actual: annoyance\n",
      "\n",
      "Text: Unbelievable.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted: ['neutral'], Actual: annoyance\n",
      "\n",
      "Text: This was super annoying.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted: ['anger', 'annoyance'], Actual: annoyance\n",
      "\n",
      "Text: It's frustrating when things don't go as planned and it's been one setback after another today.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['anger', 'annoyance', 'neutral'], Actual: annoyance\n",
      "\n",
      "Text: I didn’t expect this to work out, but I’m glad to see it’s exceeded expectations.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step \n",
      "Predicted: ['admiration', 'joy'], Actual: approval\n",
      "\n",
      "Text: I’m fully behind this—great job.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted: ['gratitude', 'neutral'], Actual: approval\n",
      "\n",
      "Text: After carefully considering everything, I can confidently say I approve of the way this project is shaping up—it’s exactly what we needed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Predicted: ['neutral'], Actual: approval\n",
      "\n",
      "Text: This looks great!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['admiration'], Actual: approval\n",
      "\n",
      "Text: Everything checks out, and I’m fully on board with how things are processing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['neutral'], Actual: approval\n",
      "\n",
      "Text: It means a lot to me that you took the time to listen and offer support, especially when I wasn’t sure what I needed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['neutral'], Actual: caring\n",
      "\n",
      "Text: That was so thoughtful of you.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['neutral'], Actual: caring\n",
      "\n",
      "Text: Thank you!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['gratitude'], Actual: caring\n",
      "\n",
      "Text: The small gestures often show the most care, and I really appreciate you always looking out for me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['admiration'], Actual: caring\n",
      "\n",
      "Text: I thought I had everything under control, but as I got deeper into the situation, it became increasingly clear that I was completely out of my depth\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted: ['approval', 'realization'], Actual: confusion\n",
      "\n",
      "Text: I’m not sure what’s happening here.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['confusion', 'curiosity', 'disapproval'], Actual: confusion\n",
      "\n",
      "Text: Not entirely sure why Whisper doesn't work on Google Collab.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['neutral'], Actual: confusion\n",
      "\n",
      "Text: The instructions were so unclear, I couldn’t tell if I was doing the right thing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['disapproval'], Actual: confusion\n",
      "\n",
      "Text: I couldn’t help but wonder how things would play out if we took a different approach—I’m really curious to see where this path leads.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted: ['curiosity', 'surprise'], Actual: curiosity\n",
      "\n",
      "Text: I need to know more about this!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['neutral'], Actual: curiosity\n",
      "\n",
      "Text: I’m dying to know how they pulled this off\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Predicted: ['neutral'], Actual: curiosity\n",
      "\n",
      "Text: There’s something that doesn’t add up, and I’m curious to find out the truth.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted: ['curiosity', 'neutral'], Actual: curiosity\n",
      "\n",
      "Text: I've been thinking about trying the new restaurant for weeks. After hearing such great things I can't wait to see if it lives up to the hype\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['admiration'], Actual: desire\n",
      "\n",
      "Text: I’ve been craving a quiet weekend getaway for so long, and now that it’s finally happening, I can’t wait to escape.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['admiration'], Actual: desire\n",
      "\n",
      "Text: I really want to see that movie.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted: ['desire', 'optimism'], Actual: desire\n",
      "\n",
      "Text: That movie was such a letdown.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['neutral'], Actual: disappointment\n",
      "\n",
      "Text: I’m bummed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Predicted: ['neutral'], Actual: disappointment\n",
      "\n",
      "Text: I was really looking forward to this event, but when I arrived and saw how poorly organized it was, I couldn’t help but feel let down.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted: ['neutral'], Actual: disappointment\n",
      "\n",
      "Text: I expected better from this product, but it didn’t quite meet my expectations, and I’m left feeling disappointed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['disappointment', 'sadness'], Actual: disappointment\n",
      "\n",
      "Text: That movie was terrible, just awful\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['disgust', 'fear'], Actual: disapproval \n",
      "\n",
      "Text: Not ideal.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['disapproval', 'neutral'], Actual: disapproval \n",
      "\n",
      "Text: I don’t think this is the right choice.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['disapproval', 'neutral'], Actual: disapproval \n",
      "\n",
      "Text: I wasn’t impressed with how this was handled—it doesn’t align with the standards I expect.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted: ['admiration'], Actual: disapproval \n",
      "\n",
      "Text: The presentation was so poorly organized that it was hard to focus on anything—nothing about it felt professional or well thought out.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['neutral'], Actual: disgust\n",
      "\n",
      "Text: Honestly didn’t expect it to be this bad—the whole thing left a bad taste in my mouth\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['disappointment'], Actual: disgust\n",
      "\n",
      "Text: That was hard to watch.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: disgust\n",
      "\n",
      "Text: So off-putting.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted: ['neutral'], Actual: disgust\n",
      "\n",
      "Text: There’s no way I can get past how poorly this turned out.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['disapproval'], Actual: disgust\n",
      "\n",
      "Text: When I tripped in front of everyone, my face turned red. Sometimes, all you can do is laugh it off.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted: ['amusement', 'approval', 'neutral'], Actual: embarrassment\n",
      "\n",
      "Text: That was so awkward.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted: ['annoyance'], Actual: embarrassment\n",
      "\n",
      "Text: It’s hard to recover from that kind of slip-up, but I did my best to move on quickly.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['admiration', 'neutral'], Actual: embarrassment\n",
      "\n",
      "Text: I tried to keep my composure, but when I was called out, I couldn’t help but feel mortified.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['disappointment', 'sadness'], Actual: embarrassment\n",
      "\n",
      "Text: When I walked into the concert hall and felt the energy in the crowd, I knew this was going to be an unforgettable experience\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted: ['admiration'], Actual: excitement\n",
      "\n",
      "Text: The game was packed with surprises, and the adrenaline kept me glued to the screen for hours\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Predicted: ['surprise'], Actual: excitement\n",
      "\n",
      "Text: The new software update was smooth and fast, I’m really pumped about this improvements\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "Predicted: ['neutral'], Actual: excitement\n",
      "\n",
      "Text: We just wrapped up an incredible trip, and every moment was filled with so much energy and excitement\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted: ['admiration'], Actual: excitement\n",
      "\n",
      "Text: After hearing about this release for weeks, I finally saw it live\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['neutral'], Actual: excitement\n",
      "\n",
      "Text: The suspense was unbearable as we waited for the results to come in. With each passing moment, I couldn’t shake the sense of dread building inside me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['neutral'], Actual: fear\n",
      "\n",
      "Text: It wasn’t just the unknown that scared me; it was the feeling that something could go horribly wrong at any moment.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['fear'], Actual: fear\n",
      "\n",
      "Text: I’m not sure I can handle this\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['approval', 'neutral'], Actual: fear\n",
      "\n",
      "Text: I’m terrified.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['fear'], Actual: fear\n",
      "\n",
      "Text: I appreciate this more than you know.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted: ['admiration', 'gratitude'], Actual: gratitude\n",
      "\n",
      "Text: I can’t thank you enough for all the support you’ve given me—it’s hard to express how much it means that you’ve always been there for me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted: ['gratitude'], Actual: gratitude\n",
      "\n",
      "Text: I’m so thankful for everything you’ve done for me—it really made a difference during a tough time.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted: ['gratitude'], Actual: gratitude\n",
      "\n",
      "Text: I’ve been trying to stay strong, but it’s hard when grief seems to follow me everywhere.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['approval', 'neutral'], Actual: grief\n",
      "\n",
      "Text: Even in the busiest moments, there’s a lingering feeling of loss that I can’t shake off.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['sadness'], Actual: grief\n",
      "\n",
      "Text: I can begin to describe how much this means to me. It's more than just a failing. It's something deep inside that can't put into words\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['annoyance'], Actual: love\n",
      "\n",
      "Text: Every little thing you do just makes me love you more\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['love'], Actual: love\n",
      "\n",
      "Text: I didn’t expect to feel so loved, but every little gesture shows how much you care.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['admiration'], Actual: love\n",
      "\n",
      "Text: I could feel my palms sweating as I prepared for the meeting, and despite the preparation, the nerves were still there, racing through my mind.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['optimism'], Actual: nervousness\n",
      "\n",
      "Text: The uncertainty of it all left me feeling on edge\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['neutral'], Actual: nervousness\n",
      "\n",
      "Text: After everything we’ve been through, I can’t help but feel proud of what we’ve accomplished together\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['sadness'], Actual: pride\n",
      "\n",
      "Text: Looking at how much we’ve achieved, I feel an overwhelming sense of pride in what we’ve done.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['admiration'], Actual: pride\n",
      "\n",
      "Text: We did it!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['neutral'], Actual: pride\n",
      "\n",
      "Text: I can’t help but smile when I think about what we’ve achieved\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted: ['neutral'], Actual: pride\n",
      "\n",
      "Text: As the pieces started to fall into place, I had a sudden realization that what I had been searching for was right in front of me the whole time.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['realization', 'neutral'], Actual: realization\n",
      "\n",
      "Text: Oh, I get it now!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['surprise'], Actual: realization\n",
      "\n",
      "Text: It didn’t hit me until that moment, but I realized I’d been missing the obvious solution all along.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['realization'], Actual: realization\n",
      "\n",
      "Text: After all the uncertainty and waiting, it was such a relief to finally see everything fall into place and know it was all going to be okay.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['admiration', 'excitement', 'joy'], Actual: relief\n",
      "\n",
      "Text: When I heard the good news, it was like a weight had been lifted off my shoulders\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['admiration', 'neutral'], Actual: relief\n",
      "\n",
      "Text: Looking back, I realize I should have handled things differently, and the weight of my decision continues to weigh heavily on me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['approval', 'optimism', 'realization'], Actual: remorse\n",
      "\n",
      "Text: I’ve learned a valuable lesson, though the remorse still hangs over me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['approval'], Actual: remorse\n",
      "\n",
      "Text: If only I had taken a moment to think it through, maybe I wouldn’t feel this way now.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['realization', 'neutral'], Actual: remorse\n",
      "\n",
      "Text: As I walked through the empty house, the realization that it was all over hit me hard—the silence felt so heavy\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['neutral'], Actual: sadness\n",
      "\n",
      "Text: It’s hard to put into words, but seeing everything change left a deep sadness that I can’t quite explain.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['disappointment', 'sadness'], Actual: sadness\n",
      "\n",
      "Text: It's strange how something so familiar can bring so much sadness when it's gone\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Predicted: ['disappointment', 'fear', 'sadness'], Actual: sadness\n",
      "\n",
      "Text: The goodbye was harder than I expected.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: sadness\n",
      "\n",
      "Text: This is probably the best action movie I've seen in a while. All the action sequences were incredible and well performed\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Predicted: ['admiration'], Actual: excitement\n",
      "\n",
      "Text: I've never imagined that a simple walk through the city could bring so much reflection on life. The way each street and every corner reminds me of old memories and makes me question how quickly time passes by without truly noticing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['realization'], Actual: realization\n",
      "\n",
      "Text: I didn't think I would like this, but it completely surprised m\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['surprise'], Actual: surprise\n",
      "\n",
      "Text: Amazing, the plot wasn't expected, I thought I had to figure it out, but every twist kept me guessing.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Predicted: ['admiration', 'neutral'], Actual: surprise\n",
      "\n",
      "Text: Totally unexpected! This show took me by surprise in the best way possible.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['excitement', 'surprise'], Actual: surprise\n",
      "\n",
      "Text: Totally caught me off guard\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['neutral'], Actual: surprise\n",
      "\n",
      "Text: That was way better than I expected\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted: ['neutral'], Actual: surprise\n",
      "\n",
      "Text: Joyful!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: joy\n",
      "\n",
      "Text: I cannot help but laugh—this experience has been pure joy from start to finish\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: joy\n",
      "\n",
      "Text: This was such an uplifting experience. I feel light and joyful!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['admiration'], Actual: joy\n",
      "\n",
      "Text: That concert was pure joy, the energy from the crowd and the music had me dancing all night.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['admiration', 'excitement', 'joy', 'neutral'], Actual: joy\n",
      "\n",
      "Text: Nothing ever seems to go right lately and it's getting exhausting.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['disappointment'], Actual: sadness\n",
      "\n",
      "Text: I keep trying but nothing ever changes. It's all just the same.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['approval', 'optimism', 'neutral'], Actual: disappointment\n",
      "\n",
      "Text: I don't know how much longer I can keep going like this.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['neutral'], Actual: sadness\n",
      "\n",
      "Text: It feels like I'm stuck in a loop, unable to escape.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['approval', 'neutral'], Actual: annoyance\n",
      "\n",
      "Text: The more I try the worse it gets.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Predicted: ['annoyance', 'disgust'], Actual: annoyance\n",
      "\n",
      "Text: It's impossible to keep up with everything.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: annoyance\n",
      "\n",
      "Text: I'm getting tired of repeating myself.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted: ['neutral'], Actual: annoyance\n",
      "\n",
      "Text: I keep hitting dead ends no matter what I try.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['neutral'], Actual: anger\n",
      "\n",
      "Text: Your support during this difficult time has been my anchor. I can't express how much that's meant to me.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: gratitude\n",
      "\n",
      "Text: The way you handled that challenging situation showed incredible leadership and composure.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['admiration'], Actual: admiration\n",
      "\n",
      "Text: Every time I think about the presentation tomorrow my heart starts racing and I feel completely overwhelmed.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['neutral'], Actual: fear\n",
      "\n",
      "Text: Despite the setbacks I believe we're on the right track and will achieve our goals soon.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['approval', 'excitement'], Actual: optimism\n",
      "\n",
      "Text: I spent weeks preparing for this opportunity only to have it cancelled at the last minute.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Predicted: ['realization', 'sadness'], Actual: disappointment\n",
      "\n",
      "Text: Finally submitting that project felt like putting down a heavy backpack I'd been carrying for months.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['approval', 'optimism', 'neutral'], Actual: relief\n",
      "\n",
      "Text: The instructions keep changing and now I'm not sure what I'm supposed to be focusing on.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['neutral'], Actual: confusion\n",
      "\n",
      "Text: Just got the email confirming our vacation plans—I can't wait to finally see the ocean!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['neutral'], Actual: excitement\n",
      "\n",
      "Text: When my phone started ringing during the speech I wanted to disappear into the floor.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "Predicted: ['neutral'], Actual: embarrassment\n",
      "\n",
      "Text: I never expected them to remember my birthday let alone plan this entire celebration.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['neutral'], Actual: surprise\n",
      "\n",
      "Text: It's been a year since we lost him but some days the pain feels just as fresh as the first day.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted: ['neutral'], Actual: grief\n",
      "\n",
      "Text: The dog's reaction to seeing snow for the first time had us laughing until we cried.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['amusement', 'sadness'], Actual: amusement\n",
      "\n",
      "Text: The state of the kitchen after my roommates' party was absolutely revolting.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted: ['admiration'], Actual: disgust\n",
      "\n",
      "Text: I've always wondered how they create those incredible special effects in movies.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['admiration', 'surprise'], Actual: curiosity\n",
      "\n",
      "Text: My hands won't stop shaking and I keep forgetting what I was about to say.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['neutral'], Actual: nervousness\n",
      "\n",
      "Text: After reviewing all the proposals yours stands out as the most comprehensive and well-researched.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Predicted: ['neutral'], Actual: approval\n",
      "\n",
      "Text: Looking through these old photographs brings back memories that make my heart ache.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['sadness'], Actual: sadness\n",
      "\n",
      "Text: Dancing in the rain with my best friends today was the most freeing feeling I've had in years.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted: ['admiration'], Actual: joy\n",
      "\n",
      "Text: If I could go back and change how I responded to her I absolutely would.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted: ['neutral'], Actual: remorse\n",
      "\n",
      "Text: Watching you with our child makes my heart feel so full it might burst.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['sadness', 'neutral'], Actual: love\n",
      "\n",
      "Text: They promised to fix this issue three weeks ago and nothing has been done!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['approval', 'optimism', 'neutral'], Actual: anger\n",
      "\n",
      "Text: It just clicked that the solution was hiding in plain sight all along.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['disappointment'], Actual: realization\n",
      "\n",
      "Text: This approach completely disregards the feedback we received from the community.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['approval', 'neutral'], Actual: disapproval \n",
      "\n",
      "Text: I made you your favorite soup since I heard you weren't feeling well.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['admiration', 'neutral'], Actual: caring\n",
      "\n",
      "Text: Could you please stop tapping your pen? It's making it impossible to concentrate.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['approval'], Actual: annoyance\n",
      "\n",
      "Text: Seeing my daughter graduate today after everything she's overcome—I couldn't be more proud.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['admiration'], Actual: pride\n",
      "\n",
      "Text: Wait so the meeting is cancelled? But I just received a reminder about it five minutes ago.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: confusion\n",
      "\n",
      "Text: I've had my eye on that vintage record player for months—I think I'm finally going to treat myself.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted: ['admiration'], Actual: desire\n",
      "\n",
      "Text: The strange noises coming from outside my window made it impossible to fall asleep last night.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Predicted: ['neutral'], Actual: fear\n",
      "\n",
      "Text: We just got approved for our dream house! I still can't believe it's really happening.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['neutral'], Actual: excitement\n",
      "\n",
      "Text: The film had such potential but completely fell apart in the second half.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['approval', 'disapproval'], Actual: disappointment\n",
      "\n",
      "Text: I opened the box expecting the usual gift card but found tickets to my favorite band instead!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['admiration', 'excitement', 'joy'], Actual: surprise\n",
      "\n",
      "Text: The smell in the refrigerator was so bad I had to step outside to catch my breath.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['neutral'], Actual: disgust\n",
      "\n",
      "Text: Your ability to remain calm under pressure is something I've always aspired to develop.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Predicted: ['neutral'], Actual: admiration\n",
      "\n",
      "Text: I've been rehearsing this speech for days but now my mind is completely blank.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['approval', 'neutral'], Actual: nervousness\n",
      "\n",
      "Text: The way you stepped in to help without being asked showed real kindness and generosity.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['neutral'], Actual: gratitude\n",
      "\n",
      "Text: Driving past our old neighborhood brought back a flood of memories I wasn't prepared for.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted: ['disappointment', 'sadness'], Actual: sadness\n",
      "\n",
      "Text: When the dog we thought was lost forever came running back—pure joy doesn't begin to describe it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['neutral'], Actual: joy\n",
      "\n",
      "Text: I should have listened to your advice instead of being so stubborn about doing it my way.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: remorse\n",
      "\n",
      "Text: How exactly do they train dolphins to perform such complex routines? It's fascinating.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['admiration'], Actual: curiosity\n",
      "\n",
      "Text: After waiting for two hours they told us our reservation had been 'misplaced'!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['neutral'], Actual: anger\n",
      "\n",
      "Text: The committee unanimously supported your recommendation—it was clearly the right call.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['neutral'], Actual: approval\n",
      "\n",
      "Text: I called him by the wrong name twice during our entire conversation.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted: ['neutral'], Actual: embarrassment\n",
      "\n",
      "Text: Being surrounded by family during the holidays reminds me how fortunate I am to have these bonds.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted: ['joy'], Actual: love\n",
      "\n",
      "Text: I really thought this time the outcome would be different but here we are again.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted: ['neutral'], Actual: disappointment\n",
      "\n",
      "Text: When the test results came back negative I could finally breathe again.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: relief\n",
      "\n",
      "Text: I can't believe someone would leave their trash all over this beautiful hiking trail.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['admiration'], Actual: disgust\n",
      "\n",
      "Text: Completing the marathon wasn't about the time—it was about proving to myself I could do it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: pride\n",
      "\n",
      "Text: So we're supposed to submit the form online but the website says in-person submissions only?\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: confusion\n",
      "\n",
      "Text: I never expected the small café around the corner to serve the best meal I've had all year.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['admiration'], Actual: surprise\n",
      "\n",
      "Text: The turbulence on that flight was so severe I was convinced we wouldn't make it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['neutral'], Actual: fear\n",
      "\n",
      "Text: The autocorrect fail in that important email had the entire office in stitches.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['neutral'], Actual: amusement\n",
      "\n",
      "Text: It's the little things that remind me they're gone—like seeing their favorite cereal in the store.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['admiration', 'approval', 'neutral'], Actual: grief\n",
      "\n",
      "Text: Just one more chapter before bed—this book is too good to put down.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step \n",
      "Predicted: ['admiration', 'neutral'], Actual: desire\n",
      "\n",
      "Text: Could everyone please stop interrupting? I haven't been able to finish a single thought.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['neutral'], Actual: annoyance\n",
      "\n",
      "Text: I noticed you've been working late so I brought you some dinner from that place you like.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['realization'], Actual: caring\n",
      "\n",
      "Text: This policy change completely undermines everything we've been working toward.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Predicted: ['disapproval', 'neutral'], Actual: disapproval \n",
      "\n",
      "Text: I've been approaching this problem from the wrong angle—no wonder nothing was working.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted: ['surprise'], Actual: realization\n",
      "\n",
      "Text: My stomach is in knots waiting for them to announce the results.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['neutral'], Actual: nervousness\n",
      "\n",
      "Text: The forecast says rain all weekend but I'm still hopeful we'll get some sunshine for the event.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['optimism'], Actual: optimism\n",
      "\n",
      "Text: Putting away their toys knowing they've outgrown them hit me harder than I expected.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted: ['annoyance'], Actual: sadness\n",
      "\n",
      "Text: The level of detail in this painting is extraordinary—I can't stop noticing new elements.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted: ['annoyance', 'neutral'], Actual: admiration\n",
      "\n",
      "Text: I wish I had taken the time to visit more often before it was too late.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['desire', 'optimism'], Actual: remorse\n",
      "\n",
      "Text: Finding your umbrella in my bag when the storm hit reminded me how thoughtful you always are.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['neutral'], Actual: gratitude\n",
      "\n",
      "Text: The power went out during the storm and every sound in the darkness seemed threatening.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['neutral'], Actual: fear\n",
      "\n",
      "Text: Watching the sunrise from the mountaintop after hours of hiking—absolutely worth every step.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: joy\n",
      "\n",
      "Text: They made these decisions without consulting anyone who would be affected by them!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: anger\n",
      "\n",
      "Text: We just booked our tickets for that festival we've been talking about for years!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted: ['neutral'], Actual: excitement\n",
      "\n",
      "Text: The reality of the experience didn't come close to matching the hype surrounding it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['neutral'], Actual: disappointment\n",
      "\n",
      "Text: I turned the corner and there they all were waiting to surprise me—I was completely speechless.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Predicted: ['surprise', 'neutral'], Actual: surprise\n",
      "\n",
      "Text: The way they spoke about those vulnerable people showed a complete lack of empathy.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted: ['annoyance', 'neutral'], Actual: disgust\n",
      "\n",
      "Text: This is exactly the direction we should be taking—it addresses all our key concerns.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Predicted: ['approval', 'neutral'], Actual: approval\n",
      "\n",
      "Text: Wait so the deadline was yesterday? But the email clearly said next week.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['admiration', 'neutral'], Actual: confusion\n",
      "\n",
      "Text: I've always been fascinated by how certain songs can instantly transport you back in time.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Predicted: ['neutral'], Actual: curiosity\n",
      "\n",
      "Text: I realized halfway through my story that I was confusing them with someone else entirely.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['realization'], Actual: embarrassment\n",
      "\n",
      "Text: When they called to say they'd found my lost wallet with everything still inside—pure relief.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['neutral'], Actual: relief\n",
      "\n",
      "Text: The constant notifications from this app are driving me crazy.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['admiration'], Actual: annoyance\n",
      "\n",
      "Text: Watching you pursue your passion with such dedication makes me fall in love with you all over again.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Predicted: ['admiration', 'love'], Actual: love\n",
      "\n",
      "Text: Some days I still reach for the phone to call them before remembering they're gone.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Predicted: ['love'], Actual: grief\n",
      "\n",
      "Text: Seeing my name published alongside researchers I've admired for years—still can't believe it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Predicted: ['disapproval'], Actual: pride\n",
      "\n",
      "Text: I had always respected the project from afar, but seeing the final version in person made me truly admire the dedication that went into it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted: ['admiration'], Actual: admiration\n",
      "\n",
      "Text: The absurdity of the situation was so unexpected that I couldn’t help but laugh out loud, completely caught off guard.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['amusement', 'surprise'], Actual: amusement\n",
      "\n",
      "Text: When I saw how they brushed off my concerns without even a second thought, my frustration reached a boiling point.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted: ['neutral'], Actual: anger\n",
      "\n",
      "Text: I was initially skeptical, but now I can confidently say that the outcome is exactly what we needed—well done!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Predicted: ['admiration', 'approval', 'neutral'], Actual: approval\n",
      "\n",
      "Text: The more I tried to figure out what was going on, the more lost I became in the complexity of it all.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predicted: ['neutral'], Actual: confusion\n",
      "\n",
      "Text: I had high expectations, but the outcome was underwhelming in every way—it’s hard to feel anything but let down.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted: ['neutral'], Actual: disappointment\n",
      "\n",
      "Exact‐match Accuracy: 22.93%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your trained components\n",
    "# (Make sure these are already defined / loaded in your script)\n",
    "# best_model, tokenizer, emotion_columns, max_len, best_threshold\n",
    "\n",
    "# Load the CSV with text and ground‐truth emotions\n",
    "emotion_data = pd.read_csv('labels_2.csv')\n",
    "\n",
    "def predict_emotions(texts,\n",
    "                     model=best_model,\n",
    "                     tokenizer=tokenizer,\n",
    "                     emotion_cols=emotion_columns,\n",
    "                     threshold=best_threshold,\n",
    "                     max_len=max_len):\n",
    "    # Tokenize & pad\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_len,\n",
    "                           padding='post', truncating='post')\n",
    "\n",
    "    # Get probability predictions\n",
    "    y_proba = model.predict(padded)\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    # Guarantee at least one emotion per text\n",
    "    zero_rows = np.where(y_pred.sum(axis=1) == 0)[0]\n",
    "    for i in zero_rows:\n",
    "        y_pred[i, y_proba[i].argmax()] = 1\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame(y_pred, columns=emotion_cols)\n",
    "    df.insert(0, 'text', texts)\n",
    "    return df\n",
    "\n",
    "def process_row(raw_text, actual_emotion):\n",
    "    print(f\"Text: {raw_text}\")\n",
    "    df_pred = predict_emotions([raw_text])\n",
    "    # get list of predicted emotion labels\n",
    "    preds = df_pred.loc[0, emotion_columns]\n",
    "    predicted_list = preds[preds == 1].index.tolist()\n",
    "\n",
    "    print(f\"Predicted: {predicted_list}, Actual: {actual_emotion}\\n\")\n",
    "    return predicted_list, actual_emotion\n",
    "\n",
    "predicted_emotions = []\n",
    "actual_emotions = []\n",
    "\n",
    "# Iterate rows in your CSV\n",
    "for idx, row in emotion_data.iterrows():\n",
    "    text = row['text']\n",
    "    actual = row['emotion']  # assumes this is a single label per row\n",
    "    pred_list, act = process_row(text, actual)\n",
    "\n",
    "    if pred_list and act is not None:\n",
    "        predicted_emotions.append(pred_list)\n",
    "        actual_emotions.append(act)\n",
    "\n",
    "# For multi‐label, exact‐match accuracy:\n",
    "matches = [1 if actual_emotions[i] in predicted_emotions[i] else 0\n",
    "           for i in range(len(actual_emotions))]\n",
    "accuracy = sum(matches) / len(matches)\n",
    "print(f\"Exact‐match Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotions(texts, model=best_model, emotion_cols=emotion_columns, tokenizer=tokenizer, threshold=best_threshold):\n",
    "    # Preprocess the texts\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(padded_sequences)\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    #print(y_pred_proba)\n",
    "    # Ensure at least one emotion is predicted for each text\n",
    "    zero_rows = np.where(np.sum(y_pred, axis=1) == 0)[0]\n",
    "    for row in zero_rows:\n",
    "        max_prob_idx = np.argmax(y_pred_proba[row])\n",
    "        y_pred[row, max_prob_idx] = 1\n",
    "\n",
    "    # Create results as a DataFrame\n",
    "    results = pd.DataFrame(y_pred, columns=emotion_cols)\n",
    "    results.insert(0, 'text', texts)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Recording started — press Ctrl+C to stop\n",
      "\n",
      "Transcribed: I'm glad the project is done.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Index(['admiration', 'approval', 'disapproval', 'neutral'], dtype='object')\n",
      "\n",
      "* Recording stopped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcribed: Oh good. Yeah.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Index(['neutral'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import numpy as np\n",
    "import queue\n",
    "import threading\n",
    "import pyaudio\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "RECORD_SECONDS = 3\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "def process_audio():\n",
    "    while True:\n",
    "        audio_data = audio_queue.get()\n",
    "        # convert to float32 in [-1,1]\n",
    "        audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "        # Whisper transcription\n",
    "        result = whisper_model.transcribe(audio_np, fp16=False)\n",
    "        text = result[\"text\"].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTranscribed: {text}\")\n",
    "\n",
    "        # Sentiment\n",
    "        sentiment = predict_emotions(text, model=best_model, emotion_cols=emotion_columns, tokenizer=tokenizer, threshold=best_threshold)\n",
    "        predicted_emotions = sentiment.loc[:, (sentiment != 0).any(axis=0)]\n",
    "        #get columnnames of predicted_emotions\n",
    "        print(predicted_emotions.columns[1:])\n",
    "    \n",
    "\n",
    "\n",
    "# start background thread\n",
    "threading.Thread(target=process_audio, daemon=True).start()\n",
    "\n",
    "try:\n",
    "    print(\"* Recording started — press Ctrl+C to stop\")\n",
    "    while True:\n",
    "        frames = []\n",
    "        for _ in range(int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            frames.append(data)\n",
    "        audio_queue.put(b\"\".join(frames))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n* Recording stopped\")\n",
    "\n",
    "finally:\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2a9Vnl0K5tW8",
    "CHKSpd4C_glj",
    "acTr8Uo-BCm2",
    "MdT2zZZYBchh",
    "D5D8KZguKT9m"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
